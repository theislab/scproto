{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb564572-82df-4562-a492-30c284085e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/icb/fatemehs.hashemig/codes/interpretable-ssl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2d399-97bf-4244-b622-b7b785183404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f53e26-93c6-45ce-9885-d3842b3fc2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    }
   ],
   "source": [
    "from interpretable_ssl.datasets.hlca import *\n",
    "\n",
    "ds = HLCADataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e9fcee-e8b1-4896-b2e6-a5446e2ccaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "study\n",
       "Banovich_Kropski_2020     121894\n",
       "Barbry_Leroy_2020          74487\n",
       "Nawijn_2021                70402\n",
       "Misharin_2021              64843\n",
       "Krasnow_2020               60982\n",
       "Jain_Misharin_2021         45557\n",
       "Misharin_Budinger_2018     41220\n",
       "Meyer_2019                 35554\n",
       "Seibold_2020               33593\n",
       "Lafyatis_Rojas_2019        24181\n",
       "Teichmann_Meyer_2019       12231\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.adata.obs.study.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725c7596-5b8e-4dc6-8dab-b2eaf85e5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.adata.obs.rename(columns={'study': 'batch'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "166f82e5-a4ed-4ea4-b32b-c2c02bf9d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "data_path = os.path.join('/home/icb/fatemehs.hashemig/data/hlca/','extension_data_pooled_raw_2000genes.h5ad')\n",
    "adata = sc.read_h5ad(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084853ff-73d2-421e-837e-b13b7573f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer = adata[adata.obs.study == 'Meyer_2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77936083-d337-4d3a-aed9-7b8bac689db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>study</th>\n",
       "      <th>original_celltype_ann</th>\n",
       "      <th>condition</th>\n",
       "      <th>subject_ID</th>\n",
       "      <th>sample</th>\n",
       "      <th>cells_or_nuclei</th>\n",
       "      <th>single_cell_platform</th>\n",
       "      <th>sample_type</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>transf_ann_level_1_label</th>\n",
       "      <th>transf_ann_level_1_uncert</th>\n",
       "      <th>transf_ann_level_2_label</th>\n",
       "      <th>transf_ann_level_2_uncert</th>\n",
       "      <th>transf_ann_level_3_label</th>\n",
       "      <th>transf_ann_level_3_uncert</th>\n",
       "      <th>transf_ann_level_4_label</th>\n",
       "      <th>transf_ann_level_4_uncert</th>\n",
       "      <th>transf_ann_level_5_label</th>\n",
       "      <th>transf_ann_level_5_uncert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_3prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>AT1</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A32</td>\n",
       "      <td>WTDAtest7887999</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 3' v2</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Epithelial</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>Alveolar epithelium</td>\n",
       "      <td>2.004057e-02</td>\n",
       "      <td>AT1</td>\n",
       "      <td>1.599639e-01</td>\n",
       "      <td>3_AT1</td>\n",
       "      <td>1.599639e-01</td>\n",
       "      <td>3_AT1</td>\n",
       "      <td>1.599639e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_3prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>B_plasma_IgA</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A32</td>\n",
       "      <td>WTDAtest7887999</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 3' v2</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Immune</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Lymphoid</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>B cell lineage</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Plasma cells</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4_Plasma cells</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_3prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>Monocyte_CD16</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A32</td>\n",
       "      <td>WTDAtest7887999</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 3' v2</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Immune</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>Myeloid</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>Monocytes</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>Non-classical monocytes</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>4_Non-classical monocytes</td>\n",
       "      <td>1.192093e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_3prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>CD8_EM/EMRA</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A32</td>\n",
       "      <td>WTDAtest7887999</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 3' v2</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Immune</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>Lymphoid</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>T cell lineage</td>\n",
       "      <td>2.600126e-01</td>\n",
       "      <td>CD8 T cells</td>\n",
       "      <td>2.600126e-01</td>\n",
       "      <td>4_CD8 T cells</td>\n",
       "      <td>2.600126e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_3prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>Macro_intravascular</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A32</td>\n",
       "      <td>WTDAtest7887999</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 3' v2</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Immune</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Myeloid</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Macrophages</td>\n",
       "      <td>1.999605e-02</td>\n",
       "      <td>Interstitial macrophages</td>\n",
       "      <td>1.999605e-02</td>\n",
       "      <td>Interstitial Mph perivascular</td>\n",
       "      <td>1.999605e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_5prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>Endothelia_vascular_Cap_g</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A44</td>\n",
       "      <td>WSSS8123931</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 5' v1</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Endothelial</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>Blood vessels</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>EC arterial</td>\n",
       "      <td>1.998818e-02</td>\n",
       "      <td>3_EC arterial</td>\n",
       "      <td>1.998818e-02</td>\n",
       "      <td>3_EC arterial</td>\n",
       "      <td>1.998818e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_5prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>Endothelia_vascular_Cap_g</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A44</td>\n",
       "      <td>WSSS8123931</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 5' v1</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Endothelial</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Blood vessels</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>EC capillary</td>\n",
       "      <td>2.799884e-01</td>\n",
       "      <td>EC general capillary</td>\n",
       "      <td>2.999883e-01</td>\n",
       "      <td>4_EC general capillary</td>\n",
       "      <td>2.999883e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_5prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>NK_CD16hi</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A44</td>\n",
       "      <td>WSSS8123931</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 5' v1</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Immune</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Lymphoid</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Innate lymphoid cell NK</td>\n",
       "      <td>1.599834e-01</td>\n",
       "      <td>NK cells</td>\n",
       "      <td>1.599834e-01</td>\n",
       "      <td>4_NK cells</td>\n",
       "      <td>1.599834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_5prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>Endothelia_vascular_Cap_g</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A44</td>\n",
       "      <td>WSSS8123931</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 5' v1</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Endothelial</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Blood vessels</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>EC venous</td>\n",
       "      <td>5.000141e-01</td>\n",
       "      <td>EC venous systemic</td>\n",
       "      <td>5.000141e-01</td>\n",
       "      <td>4_EC venous systemic</td>\n",
       "      <td>5.000141e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl</th>\n",
       "      <td>Meyer_2021_5prime</td>\n",
       "      <td>Meyer_2021</td>\n",
       "      <td>Endothelia_vascular_Cap_g</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>A44</td>\n",
       "      <td>WSSS8123931</td>\n",
       "      <td>cells</td>\n",
       "      <td>10X 5' v1</td>\n",
       "      <td>donor_lung</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Endothelial</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Blood vessels</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>EC arterial</td>\n",
       "      <td>4.799929e-01</td>\n",
       "      <td>3_EC arterial</td>\n",
       "      <td>4.799929e-01</td>\n",
       "      <td>3_EC arterial</td>\n",
       "      <td>4.799929e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129340 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             dataset  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021_3prime   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021_3prime   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021_3prime   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021_3prime   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021_3prime   \n",
       "...                                                              ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl        Meyer_2021_5prime   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl        Meyer_2021_5prime   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl        Meyer_2021_5prime   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl        Meyer_2021_5prime   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl        Meyer_2021_5prime   \n",
       "\n",
       "                                                        study  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl  Meyer_2021   \n",
       "...                                                       ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl        Meyer_2021   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl        Meyer_2021   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl        Meyer_2021   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl        Meyer_2021   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl        Meyer_2021   \n",
       "\n",
       "                                                       original_celltype_ann  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl                        AT1   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl               B_plasma_IgA   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl              Monocyte_CD16   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl                CD8_EM/EMRA   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl        Macro_intravascular   \n",
       "...                                                                      ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl        Endothelia_vascular_Cap_g   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl        Endothelia_vascular_Cap_g   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                        NK_CD16hi   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl        Endothelia_vascular_Cap_g   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl        Endothelia_vascular_Cap_g   \n",
       "\n",
       "                                                  condition subject_ID  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl   Healthy        A32   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl   Healthy        A32   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl   Healthy        A32   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl   Healthy        A32   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl   Healthy        A32   \n",
       "...                                                     ...        ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl         Healthy        A44   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl         Healthy        A44   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl         Healthy        A44   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl         Healthy        A44   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl         Healthy        A44   \n",
       "\n",
       "                                                            sample  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl  WTDAtest7887999   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl  WTDAtest7887999   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl  WTDAtest7887999   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl  WTDAtest7887999   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl  WTDAtest7887999   \n",
       "...                                                            ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl            WSSS8123931   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl            WSSS8123931   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl            WSSS8123931   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl            WSSS8123931   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl            WSSS8123931   \n",
       "\n",
       "                                                  cells_or_nuclei  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl           cells   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl           cells   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl           cells   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl           cells   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl           cells   \n",
       "...                                                           ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                 cells   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                 cells   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                 cells   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                 cells   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                 cells   \n",
       "\n",
       "                                                  single_cell_platform  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl            10X 3' v2   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl            10X 3' v2   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl            10X 3' v2   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl            10X 3' v2   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl            10X 3' v2   \n",
       "...                                                                ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                  10X 5' v1   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                  10X 5' v1   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                  10X 5' v1   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                  10X 5' v1   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                  10X 5' v1   \n",
       "\n",
       "                                                  sample_type   age  ...  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl  donor_lung  29.0  ...   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl  donor_lung  29.0  ...   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl  donor_lung  29.0  ...   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl  donor_lung  29.0  ...   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl  donor_lung  29.0  ...   \n",
       "...                                                       ...   ...  ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl        donor_lung  66.0  ...   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl        donor_lung  66.0  ...   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl        donor_lung  66.0  ...   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl        donor_lung  66.0  ...   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl        donor_lung  66.0  ...   \n",
       "\n",
       "                                                  transf_ann_level_1_label  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl               Epithelial   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl                   Immune   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl                   Immune   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl                   Immune   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl                   Immune   \n",
       "...                                                                    ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                    Endothelial   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                    Endothelial   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                         Immune   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                    Endothelial   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                    Endothelial   \n",
       "\n",
       "                                                   transf_ann_level_1_uncert  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl               5.960464e-08   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl               0.000000e+00   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl               1.192093e-07   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl               5.960464e-08   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl               0.000000e+00   \n",
       "...                                                                      ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                     5.960464e-08   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                     0.000000e+00   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                     0.000000e+00   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                     0.000000e+00   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                     0.000000e+00   \n",
       "\n",
       "                                                  transf_ann_level_2_label  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl      Alveolar epithelium   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl                 Lymphoid   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl                  Myeloid   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl                 Lymphoid   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl                  Myeloid   \n",
       "...                                                                    ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                  Blood vessels   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                  Blood vessels   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                       Lymphoid   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                  Blood vessels   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                  Blood vessels   \n",
       "\n",
       "                                                  transf_ann_level_2_uncert  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl              2.004057e-02   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl              0.000000e+00   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl              1.192093e-07   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl              5.960464e-08   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl              0.000000e+00   \n",
       "...                                                                     ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                    5.960464e-08   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                    0.000000e+00   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                    0.000000e+00   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                    0.000000e+00   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                    0.000000e+00   \n",
       "\n",
       "                                                  transf_ann_level_3_label  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl                      AT1   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl           B cell lineage   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl                Monocytes   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl           T cell lineage   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl              Macrophages   \n",
       "...                                                                    ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                    EC arterial   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                   EC capillary   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl        Innate lymphoid cell NK   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                      EC venous   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                    EC arterial   \n",
       "\n",
       "                                                  transf_ann_level_3_uncert  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl              1.599639e-01   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl              0.000000e+00   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl              1.192093e-07   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl              2.600126e-01   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl              1.999605e-02   \n",
       "...                                                                     ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                    1.998818e-02   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                    2.799884e-01   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                    1.599834e-01   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                    5.000141e-01   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                    4.799929e-01   \n",
       "\n",
       "                                                   transf_ann_level_4_label  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl                     3_AT1   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl              Plasma cells   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl   Non-classical monocytes   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl               CD8 T cells   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl  Interstitial macrophages   \n",
       "...                                                                     ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                   3_EC arterial   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl            EC general capillary   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                        NK cells   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl              EC venous systemic   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                   3_EC arterial   \n",
       "\n",
       "                                                  transf_ann_level_4_uncert  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl              1.599639e-01   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl              0.000000e+00   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl              1.192093e-07   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl              2.600126e-01   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl              1.999605e-02   \n",
       "...                                                                     ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                    1.998818e-02   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                    2.999883e-01   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                    1.599834e-01   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                    5.000141e-01   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                    4.799929e-01   \n",
       "\n",
       "                                                        transf_ann_level_5_label  \\\n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl                          3_AT1   \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl                 4_Plasma cells   \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl      4_Non-classical monocytes   \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl                  4_CD8 T cells   \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl  Interstitial Mph perivascular   \n",
       "...                                                                          ...   \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                        3_EC arterial   \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl               4_EC general capillary   \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                           4_NK cells   \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                 4_EC venous systemic   \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                        3_EC arterial   \n",
       "\n",
       "                                                  transf_ann_level_5_uncert  \n",
       "AAACCTGGTGTGAATA-1-WTDAtest7887999-0_meyer_unpubl              1.599639e-01  \n",
       "AAACGGGCAACCGCCA-1-WTDAtest7887999-0_meyer_unpubl              0.000000e+00  \n",
       "AACCATGGTACAGCAG-1-WTDAtest7887999-0_meyer_unpubl              1.192093e-07  \n",
       "AACGTTGGTGTCCTCT-1-WTDAtest7887999-0_meyer_unpubl              2.600126e-01  \n",
       "AACTCCCTCCTAGTGA-1-WTDAtest7887999-0_meyer_unpubl              1.999605e-02  \n",
       "...                                                                     ...  \n",
       "TTTGTCACAATGACCT-WSSS8123931-0_meyer_unpubl                    1.998818e-02  \n",
       "TTTGTCAGTCCGTCAG-WSSS8123931-0_meyer_unpubl                    2.999883e-01  \n",
       "TTTGTCAGTTGCGCAC-WSSS8123931-0_meyer_unpubl                    1.599834e-01  \n",
       "TTTGTCATCAGTTCGA-WSSS8123931-0_meyer_unpubl                    5.000141e-01  \n",
       "TTTGTCATCGAATCCA-WSSS8123931-0_meyer_unpubl                    4.799929e-01  \n",
       "\n",
       "[129340 rows x 41 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " meyer.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc866db5-9967-4264-b1dd-0f1aea07e8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = meyer.obs\n",
    "obs['sample'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f4e11f8-80c6-41f8-ad4f-faac4dd95f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer_cells = meyer.obs.original_celltype_ann.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6956c49f-7977-4a02-bcc0-26e5156212eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT1', 'B_plasma_IgA', 'Monocyte_CD16', 'CD8_EM/EMRA', 'Macro_intravascular', ..., 'Deuterosomal', 'SMG_Serous', 'Schwann_nonmyelinating', 'Chondrocyte', 'Schwann_Myelinating']\n",
       "Length: 78\n",
       "Categories (78, object): ['AT1', 'AT2', 'B_memory', 'B_naive', ..., 'Secretory_Goblet', 'Suprabasal', 'T_reg', 'gdT']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meyer_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ecb635c-9317-440d-ae43-09794ffe222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "study\n",
      "original_celltype_ann\n",
      "condition\n",
      "subject_ID\n",
      "sample\n",
      "cells_or_nuclei\n",
      "single_cell_platform\n",
      "sample_type\n",
      "age\n",
      "sex\n",
      "BMI\n",
      "smoking_status\n",
      "anatomical_region_level_1\n",
      "anatomical_region_coarse\n",
      "anatomical_region_detailed\n",
      "original_ann_level_1\n",
      "original_ann_level_2\n",
      "original_ann_level_3\n",
      "original_ann_level_4\n",
      "original_ann_level_5\n",
      "original_ann_level_1_clean\n",
      "original_ann_level_2_clean\n",
      "original_ann_level_3_clean\n",
      "original_ann_level_4_clean\n",
      "original_ann_level_5_clean\n",
      "core_or_extension\n",
      "reference_genome_coarse\n",
      "3'_or_5'\n",
      "ancestry\n",
      "tissue_dissociation_protocol\n",
      "transf_ann_level_1_label\n",
      "transf_ann_level_1_uncert\n",
      "transf_ann_level_2_label\n",
      "transf_ann_level_2_uncert\n",
      "transf_ann_level_3_label\n",
      "transf_ann_level_3_uncert\n",
      "transf_ann_level_4_label\n",
      "transf_ann_level_4_uncert\n",
      "transf_ann_level_5_label\n",
      "transf_ann_level_5_uncert\n"
     ]
    }
   ],
   "source": [
    "for col in obs.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393c93f5-0005-460d-95ce-7b8bffbaf685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.original_ann_level_5_clean.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a71e32f-44d6-4e87-b47d-04cbd74d3b8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suspension_type\n",
      "donor_id\n",
      "is_primary_data\n",
      "assay_ontology_term_id\n",
      "cell_type_ontology_term_id\n",
      "development_stage_ontology_term_id\n",
      "disease_ontology_term_id\n",
      "self_reported_ethnicity_ontology_term_id\n",
      "tissue_ontology_term_id\n",
      "organism_ontology_term_id\n",
      "sex_ontology_term_id\n",
      "BMI\n",
      "age_or_mean_of_age_range\n",
      "age_range\n",
      "anatomical_region_ccf_score\n",
      "ann_coarse_for_GWAS_and_modeling\n",
      "ann_finest_level\n",
      "ann_level_1\n",
      "ann_level_2\n",
      "ann_level_3\n",
      "ann_level_4\n",
      "ann_level_5\n",
      "cause_of_death\n",
      "dataset\n",
      "entropy_dataset_leiden_3\n",
      "entropy_original_ann_level_1_leiden_3\n",
      "entropy_original_ann_level_2_clean_leiden_3\n",
      "entropy_original_ann_level_3_clean_leiden_3\n",
      "entropy_subject_ID_leiden_3\n",
      "fresh_or_frozen\n",
      "leiden_1\n",
      "leiden_2\n",
      "leiden_3\n",
      "leiden_4\n",
      "leiden_5\n",
      "log10_total_counts\n",
      "lung_condition\n",
      "mixed_ancestry\n",
      "n_genes_detected\n",
      "original_ann_highest_res\n",
      "original_ann_level_1\n",
      "original_ann_level_2\n",
      "original_ann_level_3\n",
      "original_ann_level_4\n",
      "original_ann_level_5\n",
      "original_ann_nonharmonized\n",
      "reannotation_type\n",
      "reference_genome\n",
      "sample\n",
      "scanvi_label\n",
      "sequencing_platform\n",
      "size_factors\n",
      "smoking_status\n",
      "study\n",
      "subject_type\n",
      "tissue_dissociation_protocol\n",
      "tissue_level_2\n",
      "tissue_level_3\n",
      "tissue_sampling_method\n",
      "tissue_type\n",
      "cell_type\n",
      "assay\n",
      "disease\n",
      "organism\n",
      "sex\n",
      "tissue\n",
      "self_reported_ethnicity\n",
      "development_stage\n",
      "observation_joinid\n"
     ]
    }
   ],
   "source": [
    "cols = ds.adata.obs.columns\n",
    "for col in cols:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0678ff9c-1b77-4662-ba07-46f3f3c47687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlca_obs = ds.adata.obs\n",
    "hlca_obs.ann_level_5.nunique()obs.original_ann_level_5_clean.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "025a3ce4-2b54-4e77-89b6-cfb3bced65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = set(obs.original_ann_level_5_clean.unique()) - set(hlca_obs.ann_level_5.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0a2c54d-aad9-4c6c-9f38-d1ffc4db6afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ff7b7b7-3574-449a-a944-cfab50e00d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['suspension_type', 'donor_id', 'is_primary_data',\n",
       "       'assay_ontology_term_id', 'cell_type_ontology_term_id',\n",
       "       'development_stage_ontology_term_id', 'disease_ontology_term_id',\n",
       "       'self_reported_ethnicity_ontology_term_id', 'tissue_ontology_term_id',\n",
       "       'organism_ontology_term_id', 'sex_ontology_term_id', 'BMI',\n",
       "       'age_or_mean_of_age_range', 'age_range', 'anatomical_region_ccf_score',\n",
       "       'ann_coarse_for_GWAS_and_modeling', 'ann_finest_level', 'ann_level_1',\n",
       "       'ann_level_2', 'ann_level_3', 'ann_level_4', 'ann_level_5',\n",
       "       'cause_of_death', 'dataset', 'entropy_dataset_leiden_3',\n",
       "       'entropy_original_ann_level_1_leiden_3',\n",
       "       'entropy_original_ann_level_2_clean_leiden_3',\n",
       "       'entropy_original_ann_level_3_clean_leiden_3',\n",
       "       'entropy_subject_ID_leiden_3', 'fresh_or_frozen', 'leiden_1',\n",
       "       'leiden_2', 'leiden_3', 'leiden_4', 'leiden_5', 'log10_total_counts',\n",
       "       'lung_condition', 'mixed_ancestry', 'n_genes_detected',\n",
       "       'original_ann_highest_res', 'original_ann_level_1',\n",
       "       'original_ann_level_2', 'original_ann_level_3', 'original_ann_level_4',\n",
       "       'original_ann_level_5', 'original_ann_nonharmonized',\n",
       "       'reannotation_type', 'reference_genome', 'sample', 'scanvi_label',\n",
       "       'sequencing_platform', 'size_factors', 'smoking_status', 'study',\n",
       "       'subject_type', 'tissue_dissociation_protocol', 'tissue_level_2',\n",
       "       'tissue_level_3', 'tissue_sampling_method', 'tissue_type', 'cell_type',\n",
       "       'assay', 'disease', 'organism', 'sex', 'tissue',\n",
       "       'self_reported_ethnicity', 'development_stage', 'observation_joinid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.adata.obs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cd3e109-e69c-408a-8247-27ee4de89ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    }
   ],
   "source": [
    "ds = HLCADataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abbdf4db-5a2e-48b4-8324-7ee36be779b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2786363/4223828816.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "ds.adata.obs.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "027407c9-c2af-468b-b1b2-9694cc225b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type\n",
       "Epithelial     282065\n",
       "Immune         229496\n",
       "Endothelial     48166\n",
       "Stroma          25217\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.adata.obs.cell_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f9039c2-2f8b-4c88-90bb-a61cad73d4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "general_cell_type\n",
       "respiratory basal cell                             80113\n",
       "alveolar macrophage                                78816\n",
       "type II pneumocyte                                 62405\n",
       "club cell                                          36023\n",
       "nasal mucosa goblet cell                           35833\n",
       "ciliated columnar cell of tracheobronchial tree    35225\n",
       "CD8-positive, alpha-beta T cell                    29074\n",
       "elicited macrophage                                28223\n",
       "capillary endothelial cell                         23205\n",
       "CD4-positive, alpha-beta T cell                    21285\n",
       "classical monocyte                                 17695\n",
       "natural killer cell                                16978\n",
       "vein endothelial cell                              12975\n",
       "alveolar type 2 fibroblast cell                    10321\n",
       "CD1c-positive myeloid dendritic cell                9133\n",
       "non-classical monocyte                              8834\n",
       "type I pneumocyte                                   7937\n",
       "pulmonary artery endothelial cell                   7391\n",
       "mast cell                                           6623\n",
       "multi-ciliated epithelial cell                      5873\n",
       "alveolar type 1 fibroblast cell                     5182\n",
       "lung macrophage                                     4805\n",
       "respiratory hillock cell                            4600\n",
       "endothelial cell of lymphatic vessel                4595\n",
       "B cell                                              4511\n",
       "epithelial cell of lower respiratory tract          4393\n",
       "lung pericyte                                       3032\n",
       "tracheobronchial smooth muscle cell                 2996\n",
       "plasma cell                                         1773\n",
       "bronchial goblet cell                               1670\n",
       "bronchus fibroblast of lung                         1573\n",
       "serous secreting cell                               1472\n",
       "epithelial cell of alveolus of lung                 1440\n",
       "tracheobronchial serous cell                        1417\n",
       "acinar cell                                         1274\n",
       "tracheobronchial goblet cell                         968\n",
       "myofibroblast cell                                   716\n",
       "ionocyte                                             561\n",
       "smooth muscle cell                                   556\n",
       "plasmacytoid dendritic cell                          552\n",
       "mucus secreting cell                                 537\n",
       "T cell                                               500\n",
       "stromal cell                                         335\n",
       "conventional dendritic cell                          322\n",
       "dendritic cell                                       312\n",
       "fibroblast                                           276\n",
       "mesothelial cell                                     230\n",
       "brush cell of trachebronchial tree                   165\n",
       "lung neuroendocrine cell                             159\n",
       "hematopoietic stem cell                               60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.adata.obs.general_cell_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5e65cc0-b953-4494-888d-1d4ee15e37a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['suspension_type', 'donor_id', 'is_primary_data',\n",
       "       'assay_ontology_term_id', 'cell_type_ontology_term_id',\n",
       "       'development_stage_ontology_term_id', 'disease_ontology_term_id',\n",
       "       'self_reported_ethnicity_ontology_term_id', 'tissue_ontology_term_id',\n",
       "       'organism_ontology_term_id', 'sex_ontology_term_id', 'BMI',\n",
       "       'age_or_mean_of_age_range', 'age_range', 'anatomical_region_ccf_score',\n",
       "       'ann_coarse_for_GWAS_and_modeling', 'ann_finest_level', 'ann_level_1',\n",
       "       'ann_level_2', 'ann_level_3', 'ann_level_4', 'ann_level_5',\n",
       "       'cause_of_death', 'dataset', 'entropy_dataset_leiden_3',\n",
       "       'entropy_original_ann_level_1_leiden_3',\n",
       "       'entropy_original_ann_level_2_clean_leiden_3',\n",
       "       'entropy_original_ann_level_3_clean_leiden_3',\n",
       "       'entropy_subject_ID_leiden_3', 'fresh_or_frozen', 'leiden_1',\n",
       "       'leiden_2', 'leiden_3', 'leiden_4', 'leiden_5', 'log10_total_counts',\n",
       "       'lung_condition', 'mixed_ancestry', 'n_genes_detected',\n",
       "       'original_ann_highest_res', 'original_ann_level_1',\n",
       "       'original_ann_level_2', 'original_ann_level_3', 'original_ann_level_4',\n",
       "       'original_ann_level_5', 'original_ann_nonharmonized',\n",
       "       'reannotation_type', 'reference_genome', 'sample', 'scanvi_label',\n",
       "       'sequencing_platform', 'size_factors', 'smoking_status', 'study',\n",
       "       'subject_type', 'tissue_dissociation_protocol', 'tissue_level_2',\n",
       "       'tissue_level_3', 'tissue_sampling_method', 'tissue_type', 'cell_type',\n",
       "       'assay', 'disease', 'organism', 'sex', 'tissue',\n",
       "       'self_reported_ethnicity', 'development_stage', 'observation_joinid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.adata.obs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ec95ecc-8d97-46bb-91d6-c66df813407d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "study                   sequencing_platform                       \n",
       "Barbry_Leroy_2020       Illumina NextSeq 500                          74487\n",
       "Krasnow_2020            Illumina NovaSeq 6000                         60982\n",
       "Banovich_Kropski_2020   Illumina NovaSeq 6000 S4                      57225\n",
       "Jain_Misharin_2021      Illumina NovaSeq 6000                         45557\n",
       "Nawijn_2021             Illumina HiSeq 4000                           42361\n",
       "Misharin_Budinger_2018  Illumina HiSeq 4000                           41220\n",
       "Banovich_Kropski_2020   Illumina NovaSeq 6000 S1                      41001\n",
       "Misharin_2021           Illumina NovaSeq 6000                         36814\n",
       "Meyer_2019              Illumina HiSeq 4000                           35554\n",
       "Nawijn_2021             Illumina NovaSeq 6000                         28041\n",
       "Misharin_2021           Illumina HiSeq 4000                           28029\n",
       "Seibold_2020            Illumina NovaSeq 6000                         27778\n",
       "Lafyatis_Rojas_2019     Illumina NextSeq 500                          24181\n",
       "Banovich_Kropski_2020   Illumina NovaSeq 6000 S2                      21371\n",
       "Teichmann_Meyer_2019    Illumina HiSeq 4000                           12231\n",
       "Seibold_2020            Illumina NovaSeq 6000; Illumina HiSeq 4000     5815\n",
       "Banovich_Kropski_2020   Illumina NovaSeq 6000 SP                       1920\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = ds.adata.obs\n",
    "obs.value_counts(['study', 'sequencing_platform'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "531bb6a6-feb2-4a46-87e2-9a95aaf94185",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type                                        ann_level_1\n",
       "respiratory basal cell                           Epithelial     80113\n",
       "alveolar macrophage                              Immune         78816\n",
       "type II pneumocyte                               Epithelial     62405\n",
       "club cell                                        Epithelial     36023\n",
       "nasal mucosa goblet cell                         Epithelial     35833\n",
       "ciliated columnar cell of tracheobronchial tree  Epithelial     35225\n",
       "CD8-positive, alpha-beta T cell                  Immune         29074\n",
       "elicited macrophage                              Immune         28223\n",
       "capillary endothelial cell                       Endothelial    23205\n",
       "CD4-positive, alpha-beta T cell                  Immune         21285\n",
       "classical monocyte                               Immune         17695\n",
       "natural killer cell                              Immune         16978\n",
       "vein endothelial cell                            Endothelial    12975\n",
       "alveolar type 2 fibroblast cell                  Stroma         10321\n",
       "CD1c-positive myeloid dendritic cell             Immune          9133\n",
       "non-classical monocyte                           Immune          8834\n",
       "type I pneumocyte                                Epithelial      7937\n",
       "pulmonary artery endothelial cell                Endothelial     7391\n",
       "mast cell                                        Immune          6623\n",
       "multi-ciliated epithelial cell                   Epithelial      5873\n",
       "alveolar type 1 fibroblast cell                  Stroma          5182\n",
       "lung macrophage                                  Immune          4805\n",
       "respiratory hillock cell                         Epithelial      4600\n",
       "endothelial cell of lymphatic vessel             Endothelial     4595\n",
       "B cell                                           Immune          4511\n",
       "epithelial cell of lower respiratory tract       Epithelial      4393\n",
       "lung pericyte                                    Stroma          3032\n",
       "tracheobronchial smooth muscle cell              Stroma          2996\n",
       "plasma cell                                      Immune          1773\n",
       "bronchial goblet cell                            Epithelial      1670\n",
       "bronchus fibroblast of lung                      Stroma          1573\n",
       "serous secreting cell                            Epithelial      1472\n",
       "epithelial cell of alveolus of lung              Epithelial      1440\n",
       "tracheobronchial serous cell                     Epithelial      1417\n",
       "acinar cell                                      Epithelial      1274\n",
       "tracheobronchial goblet cell                     Epithelial       968\n",
       "myofibroblast cell                               Stroma           716\n",
       "ionocyte                                         Epithelial       561\n",
       "smooth muscle cell                               Stroma           556\n",
       "plasmacytoid dendritic cell                      Immune           552\n",
       "mucus secreting cell                             Epithelial       537\n",
       "T cell                                           Immune           500\n",
       "stromal cell                                     Stroma           335\n",
       "conventional dendritic cell                      Immune           322\n",
       "dendritic cell                                   Immune           312\n",
       "fibroblast                                       Stroma           276\n",
       "mesothelial cell                                 Stroma           230\n",
       "brush cell of trachebronchial tree               Epithelial       165\n",
       "lung neuroendocrine cell                         Epithelial       159\n",
       "hematopoietic stem cell                          Immune            60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.value_counts(['cell_type', 'ann_level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e46d4-90ef-4596-9816-72126adaf7e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# check immune dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b762ffb-17da-4b9b-a8c7-2818a6733cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretable_ssl.datasets.immune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcd20a10-ba15-44fe-a7e2-104a06ea7562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['batch', 'chemistry', 'data_type', 'dpt_pseudotime', 'cell_type',\n",
       "       'mt_frac', 'n_counts', 'n_genes', 'sample_ID', 'size_factors',\n",
       "       'species', 'study', 'tissue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ImmuneDataset()\n",
    "ids.adata.obs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e039de98-54a0-474b-8917-532084d6ba7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch           study  \n",
       "10X             10X        10727\n",
       "Oetjen_U        Oetjen      3730\n",
       "Freytag         Freytag     3347\n",
       "Oetjen_P        Oetjen      3265\n",
       "Oetjen_A        Oetjen      2586\n",
       "Sun_sample4_TC  Sun         2420\n",
       "Sun_sample3_TB  Sun         2403\n",
       "Sun_sample2_KC  Sun         2281\n",
       "Sun_sample1_CS  Sun         1725\n",
       "Villani         Villani     1022\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.adata.obs.value_counts(['batch', 'study'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08841c-8ab6-4f46-840a-21bd93c2fcd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# replace only batch key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb33d4d-a5ab-4da9-9c5a-3b10e06ac6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.adata.obs.rename(columns={'sequencing_platform': 'batch'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb433b-aa8b-4547-8330-2ff0fa780598",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# debug scib metric problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f656ad0f-e11a-48b6-984a-bd69dfe40fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a43a78cf-b218-4e13-ad2c-c63ae9d38ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'interpretable_ssl.augmenters.adata_augmenter' from '/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/augmenters/adata_augmenter.py'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import interpretable_ssl.augmenters.adata_augmenter\n",
    "importlib.reload(interpretable_ssl.augmenters.adata_augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5366b3fa-2069-4187-accb-905615e8d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import interpretable_ssl.trainers.swav\n",
    "# importlib.reload(interpretable_ssl.trainers.swav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3cfa84a-4d77-4ee0-a81e-5955ffc9e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretable_ssl.trainers.swav import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b20859-b270-4320-85bf-61a7c75ed3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new base init\n",
      "loading data\n"
     ]
    }
   ],
   "source": [
    "trainer = SwAV(\n",
    "    num_prototypes=300,\n",
    "    latent_dims=8,\n",
    "    batch_size=1024,\n",
    "    augmentation_type=\"nb\",\n",
    "    epsilon=0.02,\n",
    "    cvae_loss_scaler=0.0,\n",
    "    prot_decoding_loss_scaler=0,\n",
    "    model_version=1,\n",
    "    experiment_name=\"test\",\n",
    "    fine_tuning_epochs=3,\n",
    "    pretraining_epochs=3,\n",
    "    training_type=\"semi_supervised\",\n",
    "    dataset_id=\"hlca\",\n",
    "    debug=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8114c6b8-926f-41f2-b1df-a6026aaa3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3826602-c831-4b17-9854-7a760c1c95e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09/30/24 12:47:50 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 09/30/24 12:47:50 - 0:00:00 - all_latent: None\n",
      "                                     augmentation_type: nb\n",
      "                                     base_lr: 4.8\n",
      "                                     batch_size: 1024\n",
      "                                     batch_size_version: 2\n",
      "                                     cell_type_key: cell_type\n",
      "                                     checkpoint_freq: 25\n",
      "                                     condition_key: study\n",
      "                                     crops_for_assign: [0, 1]\n",
      "                                     custom_cross_val: False\n",
      "                                     cvae_loss_scaler: 0.0\n",
      "                                     cvae_reg: 0\n",
      "                                     dataset: hlca\n",
      "                                     dataset_id: hlca\n",
      "                                     debug: True\n",
      "                                     default_values: {'dataset_id': 'pbmc-immune', 'model_name_version': 5.0, 'num_prototypes': 128, 'hidden_dim': 64, 'latent_dims': 8, 'batch_size_version': 2, 'batch_size': 512, 'fine_tuning_epochs': 0, 'custom_cross_val': False, 'description': '', 'experiment_name': '', 'condition_key': 'study', 'cell_type_key': 'cell_type', 'linear_eval': False, 'only_eval': False, 'use_early_stopping': False, 'debug': False, 'pretraining_epochs': 300, 'training_type': 'pretrain', 'dump_name_version': 4, 'nmb_crops': [8], 'augmentation_type': 'knn', 'size_crops': [224], 'min_scale_crops': [0.14], 'max_scale_crops': [1], 'crops_for_assign': [0, 1], 'temperature': 0.1, 'epsilon': 0.05, 'sinkhorn_iterations': 3, 'feat_dim': 8, 'queue_length': 0, 'epoch_queue_starts': 15, 'base_lr': 4.8, 'final_lr': 0, 'freeze_prototypes_niters': 313, 'wd': 1e-06, 'warmup_epochs': 10, 'start_warmup': 0, 'cvae_reg': 0, 'dist_url': 'env://', 'world_size': -1, 'rank': 0, 'local_rank': 0, 'workers': 10, 'checkpoint_freq': 25, 'use_fp16': False, 'sync_bn': 'pytorch', 'syncbn_process_group_size': 8, 'seed': 31, 'model': '', 'optimizer': '', 'lr_schedule': '', 'queue': None, 'train_loader': '', 'training_stats': '', 'device': 'cuda', 'freezable_prototypes': False, 'cvae_loss_scaler': 0.0001, 'prot_decoding_loss_scaler': 5, 'hidden_mlp': 1024, 'swav_dim': 64, 'use_projector': False, 'model_version': 2, 'train_decoder': False, 'longest_path': 3, 'dimensionality_reduction': None, 'k_neighbors': 10}\n",
      "                                     description: None\n",
      "                                     device: cuda\n",
      "                                     dimensionality_reduction: None\n",
      "                                     dist_url: env://\n",
      "                                     dump_checkpoints: /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1/checkpoints\n",
      "                                     dump_name_version: 4\n",
      "                                     dump_path: /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1\n",
      "                                     epoch_queue_starts: 15\n",
      "                                     epsilon: 0.02\n",
      "                                     experiment_name: test\n",
      "                                     feat_dim: 8\n",
      "                                     final_lr: 0\n",
      "                                     fine_tuning_epochs: 3\n",
      "                                     finetune_ds: None\n",
      "                                     finetuning: False\n",
      "                                     fold: None\n",
      "                                     freezable_prototypes: False\n",
      "                                     freeze_prototypes_niters: 313\n",
      "                                     hidden_dim: 64\n",
      "                                     hidden_mlp: 1024\n",
      "                                     input_dim: 2000\n",
      "                                     k_neighbors: 10\n",
      "                                     latent_dims: 8\n",
      "                                     linear_eval: False\n",
      "                                     local_rank: 0\n",
      "                                     longest_path: 3\n",
      "                                     lr_schedule: None\n",
      "                                     max_scale_crops: [1]\n",
      "                                     min_scale_crops: [0.14]\n",
      "                                     model: None\n",
      "                                     model_name_version: 5.0\n",
      "                                     model_version: 1\n",
      "                                     nmb_crops: [8]\n",
      "                                     nmb_prototypes: 300\n",
      "                                     num_prototypes: 300\n",
      "                                     only_eval: False\n",
      "                                     optimizer: None\n",
      "                                     original_ref: None\n",
      "                                     partial_ref: None\n",
      "                                     pretraining_epochs: 3\n",
      "                                     prot_decoding_loss_scaler: 0\n",
      "                                     query: hlca\n",
      "                                     query_latent: None\n",
      "                                     queue: None\n",
      "                                     queue_length: 0\n",
      "                                     rank: 0\n",
      "                                     ref: hlca\n",
      "                                     ref_latent: None\n",
      "                                     seed: 31\n",
      "                                     semi_supervised: True\n",
      "                                     sinkhorn_iterations: 3\n",
      "                                     size_crops: [224]\n",
      "                                     start_warmup: 0\n",
      "                                     swav_dim: 64\n",
      "                                     sync_bn: pytorch\n",
      "                                     syncbn_process_group_size: 8\n",
      "                                     temperature: 0.1\n",
      "                                     train_augmentation: nb\n",
      "                                     train_decoder: False\n",
      "                                     train_loader: None\n",
      "                                     training_stats: None\n",
      "                                     training_type: semi_supervised\n",
      "                                     use_early_stopping: False\n",
      "                                     use_fp16: False\n",
      "                                     use_projector: False\n",
      "                                     use_projector_out: False\n",
      "                                     warmup_epochs: 10\n",
      "                                     wd: 1e-06\n",
      "                                     workers: 10\n",
      "                                     world_size: -1\n",
      "INFO - 09/30/24 12:47:50 - 0:00:00 - The experiment will be stored in /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1\n",
      "                                     \n",
      "\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby('conditions_combined').first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby(condition_keys).first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:164: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  adata.obs[cell_type_key][self.labeled_indices_]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary:\n",
      " \tNum conditions: [9]\n",
      " \tEmbedding dim: [10]\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 2000 45 10\n",
      "\tMean/Var Layer in/out: 45 8\n",
      "Decoder Architecture:\n",
      "\tFirst Layer in, out and cond:  8 45 10\n",
      "\tOutput Layer in/out:  45 2000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "INFO - 09/30/24 12:47:52 - 0:00:01 - Building data done with 548532 images loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swav model with l2n init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09/30/24 12:47:52 - 0:00:02 - SwavBase(\n",
      "                                       (scpoli_model): scpoli(\n",
      "                                         (embeddings): ModuleList(\n",
      "                                           (0): Embedding(9, 10, max_norm=1.0)\n",
      "                                         )\n",
      "                                         (encoder): Encoder(\n",
      "                                           (FC): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=2000, out_features=45, bias=True)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=45, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((45,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (mean_encoder): Linear(in_features=45, out_features=8, bias=True)\n",
      "                                           (log_var_encoder): Linear(in_features=45, out_features=8, bias=True)\n",
      "                                         )\n",
      "                                         (decoder): Decoder(\n",
      "                                           (FirstL): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=8, out_features=45, bias=False)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=45, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((45,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (HiddenL): Sequential()\n",
      "                                           (mean_decoder): Sequential(\n",
      "                                             (0): Linear(in_features=45, out_features=2000, bias=True)\n",
      "                                             (1): Softmax(dim=-1)\n",
      "                                           )\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (prototypes): Linear(in_features=8, out_features=300, bias=False)\n",
      "                                     )\n",
      "INFO - 09/30/24 12:47:52 - 0:00:02 - Building model done.\n",
      "INFO - 09/30/24 12:47:52 - 0:00:02 - Building optimizer done.\n",
      "INFO - 09/30/24 12:47:52 - 0:00:02 - no mixed precision\n"
     ]
    }
   ],
   "source": [
    "self.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4e6ff4-89ef-4e09-8645-27de2719f852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swav model with l2n init\n"
     ]
    }
   ],
   "source": [
    "ref_latent = self.encode_ref()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70747deb-68b0-404b-a097-46cedca8c6ce",
   "metadata": {},
   "source": [
    "# continue debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19362441-12c3-4f55-bb83-1765536ee621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1263c29f-02cd-45c6-940e-7abc4c457c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:In order to use the mouse gastrulation seqFISH datsets, please install squidpy (see https://github.com/scverse/squidpy).\n",
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 0\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n",
      "WARNING:root:In order to use sagenet models, please install pytorch geometric (see https://pytorch-geometric.readthedocs.io) and \n",
      " captum (see https://github.com/pytorch/captum).\n",
      "WARNING:root:mvTCR is not installed. To use mvTCR models, please install it first using \"pip install mvtcr\"\n",
      "WARNING:root:multigrate is not installed. To use multigrate models, please install it first using \"pip install multigrate\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'interpretable_ssl.augmenters.adata_augmenter' from '/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/augmenters/adata_augmenter.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import interpretable_ssl.augmenters.adata_augmenter\n",
    "importlib.reload(interpretable_ssl.augmenters.adata_augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0f7aeb-f312-4703-8f77-918fb0484551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'interpretable_ssl.datasets.hlca' from '/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/datasets/hlca.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import interpretable_ssl.datasets.hlca\n",
    "importlib.reload(interpretable_ssl.datasets.hlca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d18865eb-5bd5-412d-802e-d81ad62a7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretable_ssl.trainers.swav import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba9f8c5-0e23-491e-9ac6-a1beafc5c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new base init\n",
      "loading data\n"
     ]
    }
   ],
   "source": [
    "t = SwAV(\n",
    "    num_prototypes=300,\n",
    "    latent_dims=8,\n",
    "    batch_size=1024,\n",
    "    augmentation_type=\"nb\",\n",
    "    epsilon=0.02,\n",
    "    cvae_loss_scaler=0.0,\n",
    "    prot_decoding_loss_scaler=0,\n",
    "    model_version=1,\n",
    "    k_neighbors=10,\n",
    "    experiment_name=\"test\",\n",
    "    fine_tuning_epochs=2,\n",
    "    pretraining_epochs=2,\n",
    "    training_type=\"semi_supervised\",\n",
    "    dataset_id=\"hlca\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe79b86-c2cc-4160-8f4c-398efa76aa67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 10/01/24 07:13:32 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 10/01/24 07:13:32 - 0:00:00 - all_latent: None\n",
      "                                     augmentation_type: nb\n",
      "                                     base_lr: 4.8\n",
      "                                     batch_size: 1024\n",
      "                                     batch_size_version: 2\n",
      "                                     cell_type_key: cell_type\n",
      "                                     checkpoint_freq: 25\n",
      "                                     condition_key: study\n",
      "                                     crops_for_assign: [0, 1]\n",
      "                                     custom_cross_val: False\n",
      "                                     cvae_loss_scaler: 0.0\n",
      "                                     cvae_reg: 0\n",
      "                                     dataset: hlca\n",
      "                                     dataset_id: hlca\n",
      "                                     debug: False\n",
      "                                     default_values: {'dataset_id': 'pbmc-immune', 'model_name_version': 5.0, 'num_prototypes': 128, 'hidden_dim': 64, 'latent_dims': 8, 'batch_size_version': 2, 'batch_size': 512, 'fine_tuning_epochs': 0, 'custom_cross_val': False, 'description': '', 'experiment_name': '', 'condition_key': 'study', 'cell_type_key': 'cell_type', 'linear_eval': False, 'only_eval': False, 'use_early_stopping': False, 'debug': False, 'pretraining_epochs': 300, 'training_type': 'pretrain', 'dump_name_version': 4, 'nmb_crops': [8], 'augmentation_type': 'knn', 'size_crops': [224], 'min_scale_crops': [0.14], 'max_scale_crops': [1], 'crops_for_assign': [0, 1], 'temperature': 0.1, 'epsilon': 0.05, 'sinkhorn_iterations': 3, 'feat_dim': 8, 'queue_length': 0, 'epoch_queue_starts': 15, 'base_lr': 4.8, 'final_lr': 0, 'freeze_prototypes_niters': 313, 'wd': 1e-06, 'warmup_epochs': 10, 'start_warmup': 0, 'cvae_reg': 0, 'dist_url': 'env://', 'world_size': -1, 'rank': 0, 'local_rank': 0, 'workers': 10, 'checkpoint_freq': 25, 'use_fp16': False, 'sync_bn': 'pytorch', 'syncbn_process_group_size': 8, 'seed': 31, 'model': '', 'optimizer': '', 'lr_schedule': '', 'queue': None, 'train_loader': '', 'training_stats': '', 'device': 'cuda', 'freezable_prototypes': False, 'cvae_loss_scaler': 0.0001, 'prot_decoding_loss_scaler': 5, 'hidden_mlp': 1024, 'swav_dim': 64, 'use_projector': False, 'model_version': 2, 'train_decoder': False, 'longest_path': 3, 'dimensionality_reduction': None, 'k_neighbors': 10}\n",
      "                                     description: None\n",
      "                                     device: cuda\n",
      "                                     dimensionality_reduction: None\n",
      "                                     dist_url: env://\n",
      "                                     dump_checkpoints: /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1/checkpoints\n",
      "                                     dump_name_version: 4\n",
      "                                     dump_path: /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1\n",
      "                                     epoch_queue_starts: 15\n",
      "                                     epsilon: 0.02\n",
      "                                     experiment_name: test\n",
      "                                     feat_dim: 8\n",
      "                                     final_lr: 0\n",
      "                                     fine_tuning_epochs: 2\n",
      "                                     finetune_ds: None\n",
      "                                     finetuning: False\n",
      "                                     fold: None\n",
      "                                     freezable_prototypes: False\n",
      "                                     freeze_prototypes_niters: 313\n",
      "                                     hidden_dim: 64\n",
      "                                     hidden_mlp: 1024\n",
      "                                     input_dim: 2000\n",
      "                                     k_neighbors: 10\n",
      "                                     latent_dims: 8\n",
      "                                     linear_eval: False\n",
      "                                     local_rank: 0\n",
      "                                     longest_path: 3\n",
      "                                     lr_schedule: None\n",
      "                                     max_scale_crops: [1]\n",
      "                                     min_scale_crops: [0.14]\n",
      "                                     model: None\n",
      "                                     model_name_version: 5.0\n",
      "                                     model_version: 1\n",
      "                                     nmb_crops: [8]\n",
      "                                     nmb_prototypes: 300\n",
      "                                     num_prototypes: 300\n",
      "                                     only_eval: False\n",
      "                                     optimizer: None\n",
      "                                     original_ref: None\n",
      "                                     partial_ref: None\n",
      "                                     pretraining_epochs: 2\n",
      "                                     prot_decoding_loss_scaler: 0\n",
      "                                     query: hlca\n",
      "                                     query_latent: None\n",
      "                                     queue: None\n",
      "                                     queue_length: 0\n",
      "                                     rank: 0\n",
      "                                     ref: hlca\n",
      "                                     ref_latent: None\n",
      "                                     seed: 31\n",
      "                                     semi_supervised: True\n",
      "                                     sinkhorn_iterations: 3\n",
      "                                     size_crops: [224]\n",
      "                                     start_warmup: 0\n",
      "                                     swav_dim: 64\n",
      "                                     sync_bn: pytorch\n",
      "                                     syncbn_process_group_size: 8\n",
      "                                     temperature: 0.1\n",
      "                                     train_augmentation: nb\n",
      "                                     train_decoder: False\n",
      "                                     train_loader: None\n",
      "                                     training_stats: None\n",
      "                                     training_type: semi_supervised\n",
      "                                     use_early_stopping: False\n",
      "                                     use_fp16: False\n",
      "                                     use_projector: False\n",
      "                                     use_projector_out: False\n",
      "                                     warmup_epochs: 10\n",
      "                                     wd: 1e-06\n",
      "                                     workers: 10\n",
      "                                     world_size: -1\n",
      "INFO - 10/01/24 07:13:32 - 0:00:00 - The experiment will be stored in /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1\n",
      "                                     \n",
      "\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby('conditions_combined').first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby(condition_keys).first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:164: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  adata.obs[cell_type_key][self.labeled_indices_]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary:\n",
      " \tNum conditions: [9]\n",
      " \tEmbedding dim: [10]\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 2000 45 10\n",
      "\tMean/Var Layer in/out: 45 8\n",
      "Decoder Architecture:\n",
      "\tFirst Layer in, out and cond:  8 45 10\n",
      "\tOutput Layer in/out:  45 2000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "INFO - 10/01/24 07:13:34 - 0:00:01 - Building data done with 548532 images loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swav model with l2n init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 10/01/24 07:13:34 - 0:00:02 - SwavBase(\n",
      "                                       (scpoli_model): scpoli(\n",
      "                                         (embeddings): ModuleList(\n",
      "                                           (0): Embedding(9, 10, max_norm=1.0)\n",
      "                                         )\n",
      "                                         (encoder): Encoder(\n",
      "                                           (FC): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=2000, out_features=45, bias=True)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=45, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((45,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (mean_encoder): Linear(in_features=45, out_features=8, bias=True)\n",
      "                                           (log_var_encoder): Linear(in_features=45, out_features=8, bias=True)\n",
      "                                         )\n",
      "                                         (decoder): Decoder(\n",
      "                                           (FirstL): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=8, out_features=45, bias=False)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=45, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((45,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (HiddenL): Sequential()\n",
      "                                           (mean_decoder): Sequential(\n",
      "                                             (0): Linear(in_features=45, out_features=2000, bias=True)\n",
      "                                             (1): Softmax(dim=-1)\n",
      "                                           )\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (prototypes): Linear(in_features=8, out_features=300, bias=False)\n",
      "                                     )\n",
      "INFO - 10/01/24 07:13:34 - 0:00:02 - Building model done.\n",
      "INFO - 10/01/24 07:13:34 - 0:00:02 - Building optimizer done.\n",
      "INFO - 10/01/24 07:13:34 - 0:00:02 - no mixed precision\n"
     ]
    }
   ],
   "source": [
    "t.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac9cb48-cd0a-4031-9d54-06e575fcffb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.000, 0.000, 0.694, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.125, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000]], device='cuda:0'),\n",
       " tensor([ 6, 34, 48,  6, 39], device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b304e0-725d-4fbc-af0e-0b1a659de1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86663dfb-ec01-47ab-aa40-6933aecf9f77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[0.000, 0.000, 6.469, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 2.469, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 1.469, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 2.469, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 1.469, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 2.469, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 2.469, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.469, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 1.948, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 2.948, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 2.948, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 2.948, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 1.948, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 1.948, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 1.948, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 2.948, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.779, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.779, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 4.779, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.779, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 4.779, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.779, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 1.779, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.779, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[1.337, 0.000, 0.877, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [1.337, 0.000, 1.877, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [2.337, 0.000, 1.877, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [5.337, 0.000, 1.877, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [1.337, 0.000, 0.877, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [2.337, 0.000, 0.877, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [2.337, 0.000, 3.877, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [2.337, 0.000, 0.877, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]]]),\n",
       " 'labeled': tensor([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]], dtype=torch.float64),\n",
       " 'sizefactor': tensor([[268.521, 268.521, 268.521, 268.521, 268.521, 268.521, 268.521, 268.521],\n",
       "         [252.640, 252.640, 252.640, 252.640, 252.640, 252.640, 252.640, 252.640],\n",
       "         [185.555, 185.555, 185.555, 185.555, 185.555, 185.555, 185.555, 185.555],\n",
       "         [219.465, 219.465, 219.465, 219.465, 219.465, 219.465, 219.465, 219.465],\n",
       "         [281.384, 281.384, 281.384, 281.384, 281.384, 281.384, 281.384, 281.384],\n",
       "         [364.590, 364.590, 364.590, 364.590, 364.590, 364.590, 364.590, 364.590],\n",
       "         [166.365, 166.365, 166.365, 166.365, 166.365, 166.365, 166.365, 166.365],\n",
       "         ...,\n",
       "         [339.302, 339.302, 339.302, 339.302, 339.302, 339.302, 339.302, 339.302],\n",
       "         [242.274, 242.274, 242.274, 242.274, 242.274, 242.274, 242.274, 242.274],\n",
       "         [203.743, 203.743, 203.743, 203.743, 203.743, 203.743, 203.743, 203.743],\n",
       "         [148.093, 148.093, 148.093, 148.093, 148.093, 148.093, 148.093, 148.093],\n",
       "         [ 77.463,  77.463,  77.463,  77.463,  77.463,  77.463,  77.463,  77.463],\n",
       "         [282.735, 282.735, 282.735, 282.735, 282.735, 282.735, 282.735, 282.735],\n",
       "         [314.693, 314.693, 314.693, 314.693, 314.693, 314.693, 314.693, 314.693]]),\n",
       " 'batch': tensor([[[5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5]],\n",
       " \n",
       "         [[3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3]],\n",
       " \n",
       "         [[8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3]],\n",
       " \n",
       "         [[6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]],\n",
       " \n",
       "         [[6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6]],\n",
       " \n",
       "         [[3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3]],\n",
       " \n",
       "         [[3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3]],\n",
       " \n",
       "         [[3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3]]]),\n",
       " 'combined_batch': tensor([[5, 5, 5, 5, 5, 5, 5, 5],\n",
       "         [3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [8, 8, 8, 8, 8, 8, 8, 8],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [6, 6, 6, 6, 6, 6, 6, 6],\n",
       "         ...,\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [6, 6, 6, 6, 6, 6, 6, 6],\n",
       "         [3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [3, 3, 3, 3, 3, 3, 3, 3]]),\n",
       " 'celltypes': tensor([[[ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0]],\n",
       " \n",
       "         [[16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16]],\n",
       " \n",
       "         [[12],\n",
       "          [12],\n",
       "          [12],\n",
       "          [12],\n",
       "          [12],\n",
       "          [12],\n",
       "          [12],\n",
       "          [12]],\n",
       " \n",
       "         [[ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3]],\n",
       " \n",
       "         [[16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16]],\n",
       " \n",
       "         [[ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0]],\n",
       " \n",
       "         [[ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0]],\n",
       " \n",
       "         [[ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3]],\n",
       " \n",
       "         [[15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15]],\n",
       " \n",
       "         [[16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16]],\n",
       " \n",
       "         [[16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16]]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(self.train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c926ac98-2a7c-446a-912d-5e4265767f7c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "INFO - 10/01/24 07:15:36 - 0:02:04 - Epoch: [1][0]\tTime 12.904 (12.904)\tData 3.925 (3.925)\tLoss 9.4749 (9.4749)\tLr: 0.4801\n",
      "INFO - 10/01/24 07:15:57 - 0:02:24 - Epoch: [1][50]\tTime 0.061 (0.657)\tData 0.011 (0.421)\tLoss 6.4463 (7.6556)\tLr: 0.5250\n",
      "INFO - 10/01/24 07:16:27 - 0:02:54 - Epoch: [1][100]\tTime 0.062 (0.627)\tData 0.010 (0.484)\tLoss 4.7351 (6.5916)\tLr: 0.5698\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/trainers/swav.py:366\u001b[0m, in \u001b[0;36mSwAV.train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    363\u001b[0m use_the_queue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    365\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 366\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_time\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores, self.queue = self.train_one_epoch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be764de5-f340-4ba4-a7ee-8bc076ffb9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = self.dataset.le\n",
    "le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bc48144-c764-4956-8d8b-3eb405d21be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2c53069-dc08-4d19-8be5-a554499c64b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type\n",
       "respiratory basal cell                             80113\n",
       "alveolar macrophage                                78816\n",
       "type II pneumocyte                                 62405\n",
       "club cell                                          36023\n",
       "nasal mucosa goblet cell                           35833\n",
       "ciliated columnar cell of tracheobronchial tree    35225\n",
       "CD8-positive, alpha-beta T cell                    29074\n",
       "elicited macrophage                                28223\n",
       "capillary endothelial cell                         23205\n",
       "CD4-positive, alpha-beta T cell                    21285\n",
       "classical monocyte                                 17695\n",
       "natural killer cell                                16978\n",
       "vein endothelial cell                              12975\n",
       "alveolar type 2 fibroblast cell                    10321\n",
       "CD1c-positive myeloid dendritic cell                9133\n",
       "non-classical monocyte                              8834\n",
       "type I pneumocyte                                   7937\n",
       "pulmonary artery endothelial cell                   7391\n",
       "mast cell                                           6623\n",
       "multi-ciliated epithelial cell                      5873\n",
       "alveolar type 1 fibroblast cell                     5182\n",
       "lung macrophage                                     4805\n",
       "respiratory hillock cell                            4600\n",
       "endothelial cell of lymphatic vessel                4595\n",
       "B cell                                              4511\n",
       "epithelial cell of lower respiratory tract          4393\n",
       "lung pericyte                                       3032\n",
       "tracheobronchial smooth muscle cell                 2996\n",
       "plasma cell                                         1773\n",
       "bronchial goblet cell                               1670\n",
       "bronchus fibroblast of lung                         1573\n",
       "serous secreting cell                               1472\n",
       "epithelial cell of alveolus of lung                 1440\n",
       "tracheobronchial serous cell                        1417\n",
       "acinar cell                                         1274\n",
       "tracheobronchial goblet cell                         968\n",
       "myofibroblast cell                                   716\n",
       "ionocyte                                             561\n",
       "smooth muscle cell                                   556\n",
       "plasmacytoid dendritic cell                          552\n",
       "mucus secreting cell                                 537\n",
       "T cell                                               500\n",
       "stromal cell                                         335\n",
       "conventional dendritic cell                          322\n",
       "dendritic cell                                       312\n",
       "fibroblast                                           276\n",
       "mesothelial cell                                     230\n",
       "brush cell of trachebronchial tree                   165\n",
       "lung neuroendocrine cell                             159\n",
       "hematopoietic stem cell                               60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = self.dataset.adata.obs\n",
    "obs.cell_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46675ac0-ffce-4d18-8b5e-c08c0be4daa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alveolar macrophage',\n",
       " 'natural killer cell',\n",
       " 'type II pneumocyte',\n",
       " 'respiratory basal cell',\n",
       " 'vein endothelial cell',\n",
       " 'CD8-positive, alpha-beta T cell',\n",
       " 'pulmonary artery endothelial cell',\n",
       " 'bronchus fibroblast of lung',\n",
       " 'CD4-positive, alpha-beta T cell',\n",
       " 'type I pneumocyte',\n",
       " 'ciliated columnar cell of tracheobronchial tree',\n",
       " 'plasma cell',\n",
       " 'respiratory hillock cell',\n",
       " 'nasal mucosa goblet cell',\n",
       " 'club cell',\n",
       " 'smooth muscle cell',\n",
       " 'classical monocyte',\n",
       " 'elicited macrophage',\n",
       " 'tracheobronchial serous cell',\n",
       " 'non-classical monocyte',\n",
       " 'capillary endothelial cell',\n",
       " 'alveolar type 2 fibroblast cell',\n",
       " 'endothelial cell of lymphatic vessel',\n",
       " 'epithelial cell of lower respiratory tract',\n",
       " 'tracheobronchial smooth muscle cell',\n",
       " 'alveolar type 1 fibroblast cell',\n",
       " 'multi-ciliated epithelial cell',\n",
       " 'bronchial goblet cell',\n",
       " 'lung neuroendocrine cell',\n",
       " 'CD1c-positive myeloid dendritic cell',\n",
       " 'conventional dendritic cell',\n",
       " 'myofibroblast cell',\n",
       " 'B cell',\n",
       " 'mast cell',\n",
       " 'lung macrophage',\n",
       " 'mucus secreting cell',\n",
       " 'tracheobronchial goblet cell',\n",
       " 'lung pericyte',\n",
       " 'epithelial cell of alveolus of lung',\n",
       " 'acinar cell',\n",
       " 'mesothelial cell',\n",
       " 'serous secreting cell',\n",
       " 'ionocyte',\n",
       " 'stromal cell',\n",
       " 'brush cell of trachebronchial tree',\n",
       " 'plasmacytoid dendritic cell',\n",
       " 'T cell',\n",
       " 'fibroblast',\n",
       " 'hematopoietic stem cell',\n",
       " 'dendritic cell']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cell for cell in obs.cell_type.unique() if cell in le.classes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4812947b-6839-4121-ae82-56ce97311180",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.split_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd7a8776-80d0-489e-91e3-1626e4ce8b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493678, 54854)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(self.partial_ref), len(self.finetune_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acbdc66b-8d0f-4db3-95e3-23d5656a57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.ref = self.partial_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ab782ea-2809-4e75-bbb3-b6fe5ed0d939",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 10/01/24 07:27:48 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 10/01/24 07:27:48 - 0:00:00 - all_latent: None\n",
      "                                     augmentation_type: nb\n",
      "                                     base_lr: 4.8\n",
      "                                     batch_size: 1024\n",
      "                                     batch_size_version: 2\n",
      "                                     cell_type_key: cell_type\n",
      "                                     checkpoint_freq: 25\n",
      "                                     condition_key: study\n",
      "                                     crops_for_assign: [0, 1]\n",
      "                                     custom_cross_val: False\n",
      "                                     cvae_loss_scaler: 0.0\n",
      "                                     cvae_reg: 0\n",
      "                                     dataset: hlca\n",
      "                                     dataset_id: hlca\n",
      "                                     debug: False\n",
      "                                     default_values: {'dataset_id': 'pbmc-immune', 'model_name_version': 5.0, 'num_prototypes': 128, 'hidden_dim': 64, 'latent_dims': 8, 'batch_size_version': 2, 'batch_size': 512, 'fine_tuning_epochs': 0, 'custom_cross_val': False, 'description': '', 'experiment_name': '', 'condition_key': 'study', 'cell_type_key': 'cell_type', 'linear_eval': False, 'only_eval': False, 'use_early_stopping': False, 'debug': False, 'pretraining_epochs': 300, 'training_type': 'pretrain', 'dump_name_version': 4, 'nmb_crops': [8], 'augmentation_type': 'knn', 'size_crops': [224], 'min_scale_crops': [0.14], 'max_scale_crops': [1], 'crops_for_assign': [0, 1], 'temperature': 0.1, 'epsilon': 0.05, 'sinkhorn_iterations': 3, 'feat_dim': 8, 'queue_length': 0, 'epoch_queue_starts': 15, 'base_lr': 4.8, 'final_lr': 0, 'freeze_prototypes_niters': 313, 'wd': 1e-06, 'warmup_epochs': 10, 'start_warmup': 0, 'cvae_reg': 0, 'dist_url': 'env://', 'world_size': -1, 'rank': 0, 'local_rank': 0, 'workers': 10, 'checkpoint_freq': 25, 'use_fp16': False, 'sync_bn': 'pytorch', 'syncbn_process_group_size': 8, 'seed': 31, 'model': '', 'optimizer': '', 'lr_schedule': '', 'queue': None, 'train_loader': '', 'training_stats': '', 'device': 'cuda', 'freezable_prototypes': False, 'cvae_loss_scaler': 0.0001, 'prot_decoding_loss_scaler': 5, 'hidden_mlp': 1024, 'swav_dim': 64, 'use_projector': False, 'model_version': 2, 'train_decoder': False, 'longest_path': 3, 'dimensionality_reduction': None, 'k_neighbors': 10}\n",
      "                                     description: None\n",
      "                                     device: cuda\n",
      "                                     dimensionality_reduction: None\n",
      "                                     dist_url: env://\n",
      "                                     dump_checkpoints: /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1/checkpoints\n",
      "                                     dump_name_version: 4\n",
      "                                     dump_path: /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1\n",
      "                                     epoch_queue_starts: 15\n",
      "                                     epsilon: 0.02\n",
      "                                     experiment_name: test\n",
      "                                     feat_dim: 8\n",
      "                                     final_lr: 0\n",
      "                                     fine_tuning_epochs: 2\n",
      "                                     finetune_ds: hlca\n",
      "                                     finetuning: False\n",
      "                                     fold: None\n",
      "                                     freezable_prototypes: False\n",
      "                                     freeze_prototypes_niters: 313\n",
      "                                     hidden_dim: 64\n",
      "                                     hidden_mlp: 1024\n",
      "                                     input_dim: 2000\n",
      "                                     k_neighbors: 10\n",
      "                                     latent_dims: 8\n",
      "                                     linear_eval: False\n",
      "                                     local_rank: 0\n",
      "                                     longest_path: 3\n",
      "                                     lr_schedule: [0.00e+00 8.97e-04 1.79e-03 2.69e-03 3.59e-03 4.49e-03 5.38e-03 ...\n",
      "                                      4.79e+00 4.80e+00 4.80e+00 4.80e+00 4.80e+00 4.80e+00 4.80e+00]\n",
      "                                     max_scale_crops: [1]\n",
      "                                     min_scale_crops: [0.14]\n",
      "                                     model: SwavBase(\n",
      "                                       (scpoli_model): scpoli(\n",
      "                                         (embeddings): ModuleList(\n",
      "                                           (0): Embedding(9, 10, max_norm=1.0)\n",
      "                                         )\n",
      "                                         (encoder): Encoder(\n",
      "                                           (FC): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=2000, out_features=45, bias=True)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=45, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((45,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (mean_encoder): Linear(in_features=45, out_features=8, bias=True)\n",
      "                                           (log_var_encoder): Linear(in_features=45, out_features=8, bias=True)\n",
      "                                         )\n",
      "                                         (decoder): Decoder(\n",
      "                                           (FirstL): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=8, out_features=45, bias=False)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=45, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((45,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (HiddenL): Sequential()\n",
      "                                           (mean_decoder): Sequential(\n",
      "                                             (0): Linear(in_features=45, out_features=2000, bias=True)\n",
      "                                             (1): Softmax(dim=-1)\n",
      "                                           )\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (prototypes): Linear(in_features=8, out_features=300, bias=False)\n",
      "                                     )\n",
      "                                     model_name_version: 5.0\n",
      "                                     model_version: 1\n",
      "                                     nmb_crops: [8]\n",
      "                                     nmb_prototypes: 300\n",
      "                                     num_prototypes: 300\n",
      "                                     only_eval: False\n",
      "                                     optimizer: SGD (\n",
      "                                     Parameter Group 0\n",
      "                                         dampening: 0\n",
      "                                         differentiable: False\n",
      "                                         foreach: None\n",
      "                                         lr: 0.6066180594503645\n",
      "                                         maximize: False\n",
      "                                         momentum: 0.9\n",
      "                                         nesterov: False\n",
      "                                         weight_decay: 1e-06\n",
      "                                     )\n",
      "                                     original_ref: hlca\n",
      "                                     partial_ref: hlca\n",
      "                                     pretraining_epochs: 2\n",
      "                                     prot_decoding_loss_scaler: 0\n",
      "                                     query: hlca\n",
      "                                     query_latent: None\n",
      "                                     queue: None\n",
      "                                     queue_length: 0\n",
      "                                     rank: 0\n",
      "                                     ref: hlca\n",
      "                                     ref_latent: None\n",
      "                                     scpoli_: <scarches.models.scpoli.scpoli_model.scPoli object at 0x7f8f5ec5c800>\n",
      "                                     seed: 31\n",
      "                                     semi_supervised: True\n",
      "                                     sinkhorn_iterations: 3\n",
      "                                     size_crops: [224]\n",
      "                                     start_epoch: 0\n",
      "                                     start_warmup: 0\n",
      "                                     swav_dim: 64\n",
      "                                     sync_bn: pytorch\n",
      "                                     syncbn_process_group_size: 8\n",
      "                                     temperature: 0.1\n",
      "                                     train_augmentation: nb\n",
      "                                     train_decoder: False\n",
      "                                     train_ds: <interpretable_ssl.augmenters.adata_augmenter.MultiCropsDataset object at 0x7f8f5fa8ed20>\n",
      "                                     train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f8f5eb74d40>\n",
      "                                     training_stats: <swav.src.logger.PD_Stats object at 0x7f90643e2b40>\n",
      "                                     training_type: semi_supervised\n",
      "                                     use_early_stopping: False\n",
      "                                     use_fp16: False\n",
      "                                     use_projector: False\n",
      "                                     use_projector_out: False\n",
      "                                     warmup_epochs: 10\n",
      "                                     wd: 1e-06\n",
      "                                     workers: 10\n",
      "                                     world_size: -1\n",
      "INFO - 10/01/24 07:27:48 - 0:00:00 - The experiment will be stored in /home/icb/fatemehs.hashemig/models//hlca/test_num-prot-300_latent8-bs1024-semi_aug-nb8_ep0.02_model-v1\n",
      "                                     \n",
      "\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby('conditions_combined').first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby(condition_keys).first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:164: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  adata.obs[cell_type_key][self.labeled_indices_]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary:\n",
      " \tNum conditions: [9]\n",
      " \tEmbedding dim: [10]\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 2000 45 10\n",
      "\tMean/Var Layer in/out: 45 8\n",
      "Decoder Architecture:\n",
      "\tFirst Layer in, out and cond:  8 45 10\n",
      "\tOutput Layer in/out:  45 2000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "INFO - 10/01/24 07:27:49 - 0:00:02 - Building data done with 493678 images loaded.\n",
      "INFO - 10/01/24 07:27:49 - 0:00:02 - SwavBase(\n",
      "                                       (scpoli_model): scpoli(\n",
      "                                         (embeddings): ModuleList(\n",
      "                                           (0): Embedding(9, 10, max_norm=1.0)\n",
      "                                         )\n",
      "                                         (encoder): Encoder(\n",
      "                                           (FC): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=2000, out_features=45, bias=True)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=45, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((45,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (mean_encoder): Linear(in_features=45, out_features=8, bias=True)\n",
      "                                           (log_var_encoder): Linear(in_features=45, out_features=8, bias=True)\n",
      "                                         )\n",
      "                                         (decoder): Decoder(\n",
      "                                           (FirstL): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=8, out_features=45, bias=False)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=45, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((45,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (HiddenL): Sequential()\n",
      "                                           (mean_decoder): Sequential(\n",
      "                                             (0): Linear(in_features=45, out_features=2000, bias=True)\n",
      "                                             (1): Softmax(dim=-1)\n",
      "                                           )\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (prototypes): Linear(in_features=8, out_features=300, bias=False)\n",
      "                                     )\n",
      "INFO - 10/01/24 07:27:49 - 0:00:02 - Building model done.\n",
      "INFO - 10/01/24 07:27:49 - 0:00:02 - Building optimizer done.\n",
      "INFO - 10/01/24 07:27:49 - 0:00:02 - no mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swav model with l2n init\n"
     ]
    }
   ],
   "source": [
    "self.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d76650f3-f96c-45f2-b5a3-2c5557be48d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(self.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cca18594-dee0-4b77-a38d-741d965b335e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[ 1.621,  0.000,  2.578,  0.000,  0.000,  2.621,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 1.621,  0.000,  2.578,  0.000,  0.000,  2.621,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 1.621,  0.000,  2.578,  0.000,  0.000,  1.621,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 1.621,  0.000,  3.578,  0.000,  0.000,  1.621,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 2.621,  0.000,  2.578,  0.000,  0.000,  1.621,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 1.621,  0.000,  2.578,  0.000,  0.000,  1.621,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 3.621,  0.000,  2.578,  0.000,  0.000,  1.621,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 2.621,  0.000,  3.578,  0.000,  0.000,  1.621,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  1.403,  4.403,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  2.403,  2.403,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.403,  3.403,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.403,  2.403,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  4.403,  1.403,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  3.403,  1.403,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.403,  2.403,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.403,  1.403,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  0.000,  0.000, 21.135,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000, 10.135,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000, 11.135,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000, 24.135,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000, 10.135,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  5.135,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000, 21.135,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  3.135,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  2.264,  0.000, 12.140,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.264,  0.000, 33.140,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  4.264,  0.000,  0.140,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.264,  0.000, 13.140,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  2.264,  0.000,  3.140,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.264,  0.000,  1.140,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  2.264,  0.000,  0.140,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000, 11.264,  0.000,  6.140,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  0.000,  0.613,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  1.613,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  2.613,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  1.613,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.613,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.613,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  2.613,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.613,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  4.230,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  4.230,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.230,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.230,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.230,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.230,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  2.230,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.230,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  1.768,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  2.768,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  3.768,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  2.768,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.768,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  4.768,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.768,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  1.768,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],\n",
       " \n",
       "         [[ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],\n",
       "          [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  ...,\n",
       "            0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]]]),\n",
       " 'labeled': tensor([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]], dtype=torch.float64),\n",
       " 'sizefactor': tensor([[224.076, 224.076, 224.076, 224.076, 224.076, 224.076, 224.076, 224.076],\n",
       "         [286.428, 286.428, 286.428, 286.428, 286.428, 286.428, 286.428, 286.428],\n",
       "         [ 86.396,  86.396,  86.396,  86.396,  86.396,  86.396,  86.396,  86.396],\n",
       "         [210.272, 210.272, 210.272, 210.272, 210.272, 210.272, 210.272, 210.272],\n",
       "         [267.182, 267.182, 267.182, 267.182, 267.182, 267.182, 267.182, 267.182],\n",
       "         [222.275, 222.275, 222.275, 222.275, 222.275, 222.275, 222.275, 222.275],\n",
       "         [156.490, 156.490, 156.490, 156.490, 156.490, 156.490, 156.490, 156.490],\n",
       "         ...,\n",
       "         [176.812, 176.812, 176.812, 176.812, 176.812, 176.812, 176.812, 176.812],\n",
       "         [296.332, 296.332, 296.332, 296.332, 296.332, 296.332, 296.332, 296.332],\n",
       "         [223.681, 223.681, 223.681, 223.681, 223.681, 223.681, 223.681, 223.681],\n",
       "         [201.754, 201.754, 201.754, 201.754, 201.754, 201.754, 201.754, 201.754],\n",
       "         [210.180, 210.180, 210.180, 210.180, 210.180, 210.180, 210.180, 210.180],\n",
       "         [134.177, 134.177, 134.177, 134.177, 134.177, 134.177, 134.177, 134.177],\n",
       "         [187.202, 187.202, 187.202, 187.202, 187.202, 187.202, 187.202, 187.202]]),\n",
       " 'batch': tensor([[[3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3],\n",
       "          [3]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5]],\n",
       " \n",
       "         [[6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6],\n",
       "          [6]],\n",
       " \n",
       "         [[8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8]],\n",
       " \n",
       "         [[8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8],\n",
       "          [8]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7]],\n",
       " \n",
       "         [[7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7],\n",
       "          [7]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4]],\n",
       " \n",
       "         [[4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4]]]),\n",
       " 'combined_batch': tensor([[3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [5, 5, 5, 5, 5, 5, 5, 5],\n",
       "         [6, 6, 6, 6, 6, 6, 6, 6],\n",
       "         [8, 8, 8, 8, 8, 8, 8, 8],\n",
       "         [8, 8, 8, 8, 8, 8, 8, 8],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         ...,\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [7, 7, 7, 7, 7, 7, 7, 7],\n",
       "         [7, 7, 7, 7, 7, 7, 7, 7],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [4, 4, 4, 4, 4, 4, 4, 4],\n",
       "         [4, 4, 4, 4, 4, 4, 4, 4]]),\n",
       " 'celltypes': tensor([[[16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16],\n",
       "          [16]],\n",
       " \n",
       "         [[ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0]],\n",
       " \n",
       "         [[ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6]],\n",
       " \n",
       "         [[ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6]],\n",
       " \n",
       "         [[ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6]],\n",
       " \n",
       "         [[15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15]],\n",
       " \n",
       "         [[ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0],\n",
       "          [ 0]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[35],\n",
       "          [35],\n",
       "          [35],\n",
       "          [35],\n",
       "          [35],\n",
       "          [35],\n",
       "          [35],\n",
       "          [35]],\n",
       " \n",
       "         [[ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6]],\n",
       " \n",
       "         [[24],\n",
       "          [24],\n",
       "          [24],\n",
       "          [24],\n",
       "          [24],\n",
       "          [24],\n",
       "          [24],\n",
       "          [24]]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ed78745-a696-4132-b815-0635b592f0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13baf06c-f8fe-469c-9936-5e1ed2f11cde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "INFO - 10/01/24 07:29:48 - 0:02:01 - Epoch: [0][0]\tTime 6.411 (6.411)\tData 5.221 (5.221)\tLoss 9.4202 (9.4202)\tLr: 0.0000\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [1024, 8, 2000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "INFO - 10/01/24 07:30:15 - 0:02:28 - Epoch: [0][50]\tTime 0.047 (0.656)\tData 0.008 (0.585)\tLoss 9.0414 (9.3047)\tLr: 0.0498\n",
      "INFO - 10/01/24 07:30:44 - 0:02:56 - Epoch: [0][100]\tTime 0.035 (0.612)\tData 0.000 (0.552)\tLoss 7.6803 (8.8564)\tLr: 0.0996\n",
      "INFO - 10/01/24 07:31:08 - 0:03:21 - Epoch: [0][150]\tTime 0.058 (0.572)\tData 0.006 (0.514)\tLoss 6.7930 (8.3078)\tLr: 0.1494\n",
      "INFO - 10/01/24 07:31:37 - 0:03:50 - Epoch: [0][200]\tTime 0.051 (0.574)\tData 0.005 (0.518)\tLoss 6.1666 (7.8556)\tLr: 0.1992\n",
      "INFO - 10/01/24 07:32:06 - 0:04:18 - Epoch: [0][250]\tTime 0.060 (0.573)\tData 0.010 (0.518)\tLoss 5.6089 (7.4649)\tLr: 0.2490\n",
      "INFO - 10/01/24 07:32:33 - 0:04:45 - Epoch: [0][300]\tTime 0.061 (0.567)\tData 0.001 (0.512)\tLoss 4.8286 (7.0898)\tLr: 0.2988\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>\n",
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>Traceback (most recent call last):\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "Traceback (most recent call last):\n",
      "      File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "self._shutdown_workers()        self._shutdown_workers()\n",
      "\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "self._shutdown_workers()    \n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "        if w.is_alive():\n",
      "if w.is_alive():if w.is_alive(): \n",
      "\n",
      "                   ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^^    \n",
      "\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "    AssertionError    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
      "AssertionErrorcan only test a child process: \n",
      "AssertionError: can only test a child processcan only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>^^\n",
      "^Exception ignored in: Traceback (most recent call last):\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>^\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "^Traceback (most recent call last):\n",
      "    self._shutdown_workers()  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "^\n",
      "^      File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "^self._shutdown_workers()    ^if w.is_alive():^\n",
      "\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "  ^    \n",
      "if w.is_alive():  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      " \n",
      "      assert self._parent_pid == os.getpid(), 'can only test a child process'  \n",
      "  AssertionError :  can only test a child process^ \n",
      "^ ^ ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    ^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      ":   File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    \n",
      "AssertionErrorself._shutdown_workers()\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      ":     if w.is_alive():can only test a child process\n",
      "\n",
      " Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "      if w.is_alive():\n",
      "        ^   ^^^^^^^^^^^^^^^^^^^^\n",
      "^  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "AssertionError:     can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8f86a1ade0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "INFO - 10/01/24 07:33:02 - 0:05:14 - Epoch: [0][350]\tTime 0.074 (0.570)\tData 0.014 (0.516)\tLoss 4.0799 (6.7114)\tLr: 0.3486\n",
      "INFO - 10/01/24 07:33:30 - 0:05:43 - Epoch: [0][400]\tTime 0.040 (0.570)\tData 0.001 (0.516)\tLoss 3.4121 (6.3392)\tLr: 0.3984\n",
      "INFO - 10/01/24 07:33:57 - 0:06:09 - Epoch: [0][450]\tTime 0.067 (0.565)\tData 0.006 (0.512)\tLoss 2.8665 (5.9833)\tLr: 0.4482\n"
     ]
    }
   ],
   "source": [
    "scores, self.queue = self.train_one_epoch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e582878-49cf-4737-8ac7-7ccb8e319c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nb'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.train_ds.augmentation_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "041cd5d8-09df-4f92-8ff8-ee1d5747d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.finetuning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e98b5542-5279-423d-9a35-948c942966a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.ref = self.finetune_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48135d29-3bcb-4a07-9c38-98ef3f35b438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.ref.adata.obs.cell_type.value_counts().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe516a-34bf-40c9-a422-306d7c5fa6eb",
   "metadata": {},
   "source": [
    "# test scgraph with swav env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7499b75d-a629-4100-aa6f-b72275edbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/icb/fatemehs.hashemig/Islander/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1fe6d3a-3571-497c-a63b-2ffdb7f96773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scGraph import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05b2ee-aee8-4cea-9d51-b158bc206341",
   "metadata": {},
   "source": [
    "# prepare sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce9249bc-3a9f-49d7-af95-c7108d02fab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 10000 Ã— 400\n",
       "    obs: 'batch', 'cell_type'\n",
       "    obsm: 'X_emb1', 'X_emb2'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "\n",
    "# Create a very small example AnnData object with random data\n",
    "n_cells = 10000\n",
    "n_genes = 400\n",
    "n_batches = 3\n",
    "n_labels = 5\n",
    "n_dim_emb = 8  # Number of dimensions for embeddings\n",
    "\n",
    "# Random expression matrix (cells x genes)\n",
    "X = np.random.rand(n_cells, n_genes)\n",
    "\n",
    "# Create random metadata for batches and cell types\n",
    "batches = np.random.choice([f\"batch_{i}\" for i in range(n_batches)], n_cells)\n",
    "cell_types = np.random.choice([f\"celltype_{i}\" for i in range(n_labels)], n_cells)\n",
    "\n",
    "# Create random embeddings for emb1 and emb2 (cells x embedding dimensions)\n",
    "emb1 = np.random.rand(n_cells, n_dim_emb)\n",
    "emb2 = np.random.rand(n_cells, n_dim_emb)\n",
    "\n",
    "# Create the AnnData object\n",
    "adata_small = sc.AnnData(X)\n",
    "adata_small.obs['batch'] = batches\n",
    "adata_small.obs[\"cell_type\"] = cell_types\n",
    "\n",
    "# Add the embeddings to obsm\n",
    "adata_small.obsm[\"X_emb1\"] = emb1\n",
    "adata_small.obsm[\"X_emb2\"] = emb2\n",
    "\n",
    "adata_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79c7b5d7-faf5-41a7-b0e0-677f38dc5490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'batch' as categorical\n",
      "... storing 'cell_type' as categorical\n"
     ]
    }
   ],
   "source": [
    "# Save the AnnData object to a file\n",
    "adata_small.write(\"../sample_adata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "80244a1b-59da-4cff-a48b-33404d6fdc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type   batch  \n",
       "celltype_4  batch_1    701\n",
       "celltype_1  batch_0    694\n",
       "celltype_2  batch_2    694\n",
       "celltype_0  batch_0    691\n",
       "celltype_1  batch_1    686\n",
       "celltype_3  batch_2    684\n",
       "celltype_4  batch_0    670\n",
       "celltype_2  batch_1    669\n",
       "celltype_1  batch_2    655\n",
       "celltype_2  batch_0    655\n",
       "celltype_0  batch_2    652\n",
       "            batch_1    651\n",
       "celltype_3  batch_0    645\n",
       "            batch_1    640\n",
       "celltype_4  batch_2    613\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_small.obs.value_counts(['cell_type', 'batch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6154b2-33d2-4f5d-9807-08add589d706",
   "metadata": {},
   "source": [
    "# calculate metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e4b97d6-ead9-4b2f-8786-3be85a634bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scgraph = scGraph(\n",
    "    adata_path=\"../sample_adata.h5ad\", \n",
    "    batch_key=\"batch\", \n",
    "    label_key=\"cell_type\", \n",
    "    hvg=False, \n",
    "    trim_rate=0.05, \n",
    "    thres_batch=100, \n",
    "    thres_celltype=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6d99bf79-abea-4142-b520-041858d582b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches, calcualte centroids and pairwise distances\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1d056786a24998b07ef5479cc0f52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = scgraph.main(_obsm_list=[\"X_emb1\", \"X_emb2\"])  # Evaluate both embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83e9e340-a456-46cc-84be-d36daf0e3b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank-PCA</th>\n",
       "      <th>Corr-PCA</th>\n",
       "      <th>Corr-Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_emb1</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.881301</td>\n",
       "      <td>0.256940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_emb2</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.887659</td>\n",
       "      <td>-0.274319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rank-PCA  Corr-PCA  Corr-Weighted\n",
       "X_emb1      0.62  0.881301       0.256940\n",
       "X_emb2      0.34  0.887659      -0.274319"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98341a-cd61-4931-b2c5-cd0af5411a60",
   "metadata": {},
   "source": [
    "# test metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d77131-4a65-465a-9086-8e6672951489",
   "metadata": {},
   "source": [
    "## import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "60980ce6-9e7a-479a-8b8c-d5d54df91e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing scgraph\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import interpretable_ssl.evaluation.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4dc453d5-6864-4d83-8589-23e12e839a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing scgraph\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'interpretable_ssl.evaluation.metrics' from '/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/evaluation/metrics.py'>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(interpretable_ssl.evaluation.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "47ef0696-431a-4829-b0a4-fecff39f3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretable_ssl.evaluation.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba47f2-866d-42b5-b232-315ebb7243c1",
   "metadata": {},
   "source": [
    "## investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "09d7487a-25a2-46d9-939e-06bd727cbeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 10000 Ã— 400\n",
       "    obs: 'batch', 'cell_type'\n",
       "    obsm: 'X_emb1', 'X_emb2'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = adata_small\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4b4f04b3-3dfe-4ba3-b6d2-fa34795f2601",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MetricCalculator(adata, [adata_small.obsm['X_emb1'], adata_small.obsm['X_emb2']], ['l1', 'l2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5f7e85af-27ed-4be3-9a0e-ee747e19d417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 10000 Ã— 400\n",
       "    obs: 'batch', 'cell_type'\n",
       "    obsm: 'X_emb1', 'X_emb2', 'l1', 'l2'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "99e8985d-291a-48a1-be85-b9f4daab523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing neighbors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.02s/it]\n",
      "Embeddings:   0%|\u001b[32m          \u001b[0m| 0/2 [00:00<?, ?it/s]\n",
      "Metrics:   0%|\u001b[34m          \u001b[0m| 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Metrics:   0%|\u001b[34m          \u001b[0m| 0/10 [00:00<?, ?it/s, Bio conservation: isolated_labels]\u001b[AINFO - 10/01/24 12:08:37 - 4:40:49 - isolated labels: no more than 3 batches per label\n",
      "\n",
      "Metrics:  10%|\u001b[34mâ–ˆ         \u001b[0m| 1/10 [00:01<00:09,  1.08s/it, Bio conservation: isolated_labels]\u001b[A\n",
      "Metrics:  10%|\u001b[34mâ–ˆ         \u001b[0m| 1/10 [00:01<00:09,  1.08s/it, Bio conservation: nmi_ari_cluster_labels_kmeans]\u001b[A\n",
      "Metrics:  20%|\u001b[34mâ–ˆâ–ˆ        \u001b[0m| 2/10 [00:31<02:25, 18.20s/it, Bio conservation: nmi_ari_cluster_labels_kmeans]\u001b[A\n",
      "Metrics:  20%|\u001b[34mâ–ˆâ–ˆ        \u001b[0m| 2/10 [00:31<02:25, 18.20s/it, Bio conservation: silhouette_label]             \u001b[A\n",
      "Metrics:  30%|\u001b[34mâ–ˆâ–ˆâ–ˆ       \u001b[0m| 3/10 [00:31<01:09,  9.98s/it, Bio conservation: silhouette_label]\u001b[A\n",
      "Metrics:  30%|\u001b[34mâ–ˆâ–ˆâ–ˆ       \u001b[0m| 3/10 [00:31<01:09,  9.98s/it, Bio conservation: clisi_knn]       \u001b[A\n",
      "Metrics:  40%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆ      \u001b[0m| 4/10 [00:31<00:59,  9.98s/it, Batch correction: silhouette_batch]\u001b[A\n",
      "Metrics:  50%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     \u001b[0m| 5/10 [00:31<00:21,  4.37s/it, Batch correction: silhouette_batch]\u001b[A\n",
      "Metrics:  50%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     \u001b[0m| 5/10 [00:31<00:21,  4.37s/it, Batch correction: ilisi_knn]       \u001b[A\n",
      "Metrics:  60%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    \u001b[0m| 6/10 [00:31<00:17,  4.37s/it, Batch correction: kbet_per_label]\u001b[A\n",
      "Metrics:  70%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   \u001b[0m| 7/10 [00:37<00:11,  3.81s/it, Batch correction: kbet_per_label]\u001b[A\n",
      "Metrics:  70%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   \u001b[0m| 7/10 [00:37<00:11,  3.81s/it, Batch correction: graph_connectivity]\u001b[A\n",
      "Metrics:  80%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  \u001b[0m| 8/10 [00:37<00:07,  3.81s/it, Batch correction: pcr_comparison]    \u001b[A\n",
      "Embeddings:  50%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     \u001b[0m| 1/2 [00:38<00:38, 38.80s/it]atch correction: pcr_comparison]\u001b[A\n",
      "Metrics:   0%|\u001b[34m          \u001b[0m| 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                         \u001b[A\n",
      "Metrics:   0%|\u001b[34m          \u001b[0m| 0/10 [00:00<?, ?it/s, Bio conservation: isolated_labels]\u001b[AINFO - 10/01/24 12:09:16 - 4:41:28 - isolated labels: no more than 3 batches per label\n",
      "\n",
      "Metrics:  10%|\u001b[34mâ–ˆ         \u001b[0m| 1/10 [00:00<00:02,  3.90it/s, Bio conservation: isolated_labels]\u001b[A\n",
      "Metrics:  10%|\u001b[34mâ–ˆ         \u001b[0m| 1/10 [00:00<00:02,  3.90it/s, Bio conservation: nmi_ari_cluster_labels_kmeans]\u001b[A\n",
      "Metrics:  20%|\u001b[34mâ–ˆâ–ˆ        \u001b[0m| 2/10 [00:01<00:07,  1.13it/s, Bio conservation: nmi_ari_cluster_labels_kmeans]\u001b[A\n",
      "Metrics:  20%|\u001b[34mâ–ˆâ–ˆ        \u001b[0m| 2/10 [00:01<00:07,  1.13it/s, Bio conservation: silhouette_label]             \u001b[A\n",
      "Metrics:  30%|\u001b[34mâ–ˆâ–ˆâ–ˆ       \u001b[0m| 3/10 [00:01<00:03,  1.81it/s, Bio conservation: silhouette_label]\u001b[A\n",
      "Metrics:  30%|\u001b[34mâ–ˆâ–ˆâ–ˆ       \u001b[0m| 3/10 [00:01<00:03,  1.81it/s, Bio conservation: clisi_knn]       \u001b[A\n",
      "Metrics:  40%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆ      \u001b[0m| 4/10 [00:01<00:03,  1.81it/s, Batch correction: silhouette_batch]\u001b[A\n",
      "Metrics:  50%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     \u001b[0m| 5/10 [00:01<00:01,  3.71it/s, Batch correction: silhouette_batch]\u001b[A\n",
      "Metrics:  50%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     \u001b[0m| 5/10 [00:01<00:01,  3.71it/s, Batch correction: ilisi_knn]       \u001b[A\n",
      "Metrics:  60%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    \u001b[0m| 6/10 [00:01<00:01,  3.71it/s, Batch correction: kbet_per_label]\u001b[A\n",
      "Metrics:  70%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   \u001b[0m| 7/10 [00:07<00:04,  1.50s/it, Batch correction: kbet_per_label]\u001b[A\n",
      "Metrics:  70%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   \u001b[0m| 7/10 [00:07<00:04,  1.50s/it, Batch correction: graph_connectivity]\u001b[A\n",
      "Embeddings: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 2/2 [00:46<00:00, 23.36s/it]atch correction: pcr_comparison]    \u001b[A\n",
      "\n",
      "                                                                                         \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batches, calcualte centroids and pairwise distances\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc6d0614afc4d2c8f684ba148b0d425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Isolated labels</th>\n",
       "      <th>KMeans NMI</th>\n",
       "      <th>KMeans ARI</th>\n",
       "      <th>Silhouette label</th>\n",
       "      <th>cLISI</th>\n",
       "      <th>Silhouette batch</th>\n",
       "      <th>iLISI</th>\n",
       "      <th>KBET</th>\n",
       "      <th>Graph connectivity</th>\n",
       "      <th>PCR comparison</th>\n",
       "      <th>Batch correction</th>\n",
       "      <th>Bio conservation</th>\n",
       "      <th>Total</th>\n",
       "      <th>Rank-PCA</th>\n",
       "      <th>Corr-PCA</th>\n",
       "      <th>Corr-Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>l1</th>\n",
       "      <td>0.49791</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.497907</td>\n",
       "      <td>0.265826</td>\n",
       "      <td>0.990826</td>\n",
       "      <td>0.832735</td>\n",
       "      <td>0.954806</td>\n",
       "      <td>0.799292</td>\n",
       "      <td>0</td>\n",
       "      <td>0.715532</td>\n",
       "      <td>0.252412</td>\n",
       "      <td>0.43766</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.881301</td>\n",
       "      <td>0.256940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l2</th>\n",
       "      <td>0.497935</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.497924</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.990851</td>\n",
       "      <td>0.82874</td>\n",
       "      <td>0.946303</td>\n",
       "      <td>0.80452</td>\n",
       "      <td>0.100259</td>\n",
       "      <td>0.734135</td>\n",
       "      <td>0.253583</td>\n",
       "      <td>0.445804</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.887659</td>\n",
       "      <td>-0.274319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Isolated labels KMeans NMI KMeans ARI Silhouette label     cLISI  \\\n",
       "l1         0.49791   0.000454  -0.000035         0.497907  0.265826   \n",
       "l2        0.497935    0.00022  -0.000221         0.497924  0.272059   \n",
       "\n",
       "   Silhouette batch     iLISI      KBET Graph connectivity PCR comparison  \\\n",
       "l1         0.990826  0.832735  0.954806           0.799292              0   \n",
       "l2         0.990851   0.82874  0.946303            0.80452       0.100259   \n",
       "\n",
       "   Batch correction Bio conservation     Total  Rank-PCA  Corr-PCA  \\\n",
       "l1         0.715532         0.252412   0.43766      0.62  0.881301   \n",
       "l2         0.734135         0.253583  0.445804      0.34  0.887659   \n",
       "\n",
       "    Corr-Weighted  \n",
       "l1       0.256940  \n",
       "l2      -0.274319  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = m.calculate()\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
