{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6222b2f4-9a50-495f-846b-4f39a459cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory (where the notebook is located)\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "\n",
    "# Set the parent directory as the working directory\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "# Optionally, you can add the parent directory to the system path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Print the current working directory to verify\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0594a6bc-3850-4540-b6c1-ef871296208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14eb7f7f-c09a-4dfc-afad-55987116b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:In order to use the mouse gastrulation seqFISH datsets, please install squidpy (see https://github.com/scverse/squidpy).\n",
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 0\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n",
      "WARNING:root:In order to use sagenet models, please install pytorch geometric (see https://pytorch-geometric.readthedocs.io) and \n",
      " captum (see https://github.com/pytorch/captum).\n",
      "WARNING:root:mvTCR is not installed. To use mvTCR models, please install it first using \"pip install mvtcr\"\n",
      "WARNING:root:multigrate is not installed. To use multigrate models, please install it first using \"pip install multigrate\".\n"
     ]
    }
   ],
   "source": [
    "# import interpretable_ssl.datasets.dataset\n",
    "# import interpretable_ssl.trainers.swav\n",
    "# importlib.reload(interpretable_ssl.datasets.dataset)\n",
    "# importlib.reload(interpretable_ssl.trainers.swav)\n",
    "\n",
    "from interpretable_ssl.trainers.swav import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858a5010-782d-4f5a-be7e-e015301c7256",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/28/24 14:15:38 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 08/28/24 14:15:38 - 0:00:00 - all_latent: None\n",
      "                                     augmentation_type: scanpy_knn\n",
      "                                     base_lr: 4.8\n",
      "                                     batch_size: 512\n",
      "                                     batch_size_version: 2\n",
      "                                     cell_type_key: cell_type\n",
      "                                     checkpoint_freq: 25\n",
      "                                     condition_key: study\n",
      "                                     crops_for_assign: [0, 1]\n",
      "                                     custom_cross_val: False\n",
      "                                     cvae_loss_scaler: 0.0001\n",
      "                                     cvae_reg: 0\n",
      "                                     dataset: pbmc-immune\n",
      "                                     dataset_id: pbmc-immune\n",
      "                                     debug: False\n",
      "                                     default_values: {'dataset_id': 'pbmc-immune', 'model_name_version': 5.0, 'num_prototypes': 128, 'hidden_dim': 64, 'latent_dims': 8, 'batch_size_version': 2, 'batch_size': 512, 'fine_tuning_epochs': 0, 'custom_cross_val': False, 'description': '', 'experiment_name': '', 'condition_key': 'study', 'cell_type_key': 'cell_type', 'linear_eval': False, 'only_eval': False, 'use_early_stopping': False, 'debug': False, 'pretraining_epochs': 300, 'dump_name_version': 4, 'nmb_crops': [8], 'augmentation_type': 'knn', 'size_crops': [224], 'min_scale_crops': [0.14], 'max_scale_crops': [1], 'crops_for_assign': [0, 1], 'temperature': 0.1, 'epsilon': 0.05, 'sinkhorn_iterations': 3, 'feat_dim': 8, 'queue_length': 0, 'epoch_queue_starts': 15, 'base_lr': 4.8, 'final_lr': 0, 'freeze_prototypes_niters': 313, 'wd': 1e-06, 'warmup_epochs': 10, 'start_warmup': 0, 'cvae_reg': 0, 'dist_url': 'env://', 'world_size': -1, 'rank': 0, 'local_rank': 0, 'workers': 10, 'checkpoint_freq': 25, 'use_fp16': False, 'sync_bn': 'pytorch', 'syncbn_process_group_size': 8, 'seed': 31, 'model': '', 'optimizer': '', 'lr_schedule': '', 'queue': None, 'train_loader': '', 'training_stats': '', 'device': 'cuda', 'freezable_prototypes': False, 'cvae_loss_scaler': 0.0001, 'prot_decoding_loss_scaler': 5, 'hidden_mlp': 1024, 'swav_dim': 64, 'use_projector': False, 'model_version': 2, 'train_decoder': False, 'longest_path': 3, 'dimensionality_reduction': None, 'k_neighbors': 10}\n",
      "                                     description: None\n",
      "                                     device: cuda\n",
      "                                     dimensionality_reduction: pca\n",
      "                                     dist_url: env://\n",
      "                                     dump_checkpoints: /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-scanpy_knn8_ep0.05_dimensionality_reduction-pca/checkpoints\n",
      "                                     dump_name_version: 4\n",
      "                                     dump_path: /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-scanpy_knn8_ep0.05_dimensionality_reduction-pca\n",
      "                                     epoch_queue_starts: 15\n",
      "                                     epsilon: 0.05\n",
      "                                     experiment_name: swav-all-loss_iloss5_closs0.0001\n",
      "                                     feat_dim: 8\n",
      "                                     final_lr: 0\n",
      "                                     fine_tuning_epochs: 0\n",
      "                                     finetune_ds: None\n",
      "                                     fold: None\n",
      "                                     freezable_prototypes: False\n",
      "                                     freeze_prototypes_niters: 313\n",
      "                                     hidden_dim: 64\n",
      "                                     hidden_mlp: 1024\n",
      "                                     input_dim: 4000\n",
      "                                     k_neighbors: 10\n",
      "                                     latent_dims: 8\n",
      "                                     linear_eval: False\n",
      "                                     local_rank: 0\n",
      "                                     longest_path: 3\n",
      "                                     lr_schedule: None\n",
      "                                     max_scale_crops: [1]\n",
      "                                     min_scale_crops: [0.14]\n",
      "                                     model: None\n",
      "                                     model_name_version: 5.0\n",
      "                                     model_version: 2\n",
      "                                     nmb_crops: [8]\n",
      "                                     nmb_prototypes: 128\n",
      "                                     num_prototypes: 128\n",
      "                                     only_eval: False\n",
      "                                     optimizer: None\n",
      "                                     original_ref: None\n",
      "                                     pretraining_epochs: 300\n",
      "                                     prot_decoding_loss_scaler: 5\n",
      "                                     query: pbmc-immune\n",
      "                                     query_latent: None\n",
      "                                     queue: None\n",
      "                                     queue_length: 0\n",
      "                                     rank: 0\n",
      "                                     ref: pbmc-immune\n",
      "                                     ref_latent: None\n",
      "                                     seed: 31\n",
      "                                     sinkhorn_iterations: 3\n",
      "                                     size_crops: [224]\n",
      "                                     start_warmup: 0\n",
      "                                     swav_dim: 64\n",
      "                                     sync_bn: pytorch\n",
      "                                     syncbn_process_group_size: 8\n",
      "                                     temperature: 0.1\n",
      "                                     train_decoder: False\n",
      "                                     train_loader: None\n",
      "                                     training_stats: None\n",
      "                                     use_early_stopping: False\n",
      "                                     use_fp16: False\n",
      "                                     use_projector: False\n",
      "                                     use_projector_out: False\n",
      "                                     warmup_epochs: 10\n",
      "                                     wd: 1e-06\n",
      "                                     workers: 10\n",
      "                                     world_size: -1\n",
      "INFO - 08/28/24 14:15:38 - 0:00:00 - The experiment will be stored in /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-scanpy_knn8_ep0.05_dimensionality_reduction-pca\n",
      "                                     \n",
      "\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby('conditions_combined').first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby(condition_keys).first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:164: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  adata.obs[cell_type_key][self.labeled_indices_]\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "INFO - 08/28/24 14:15:38 - 0:00:00 - Building data done with 29137 images loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary:\n",
      " \tNum conditions: [3]\n",
      " \tEmbedding dim: [10]\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 64 10\n",
      "\tMean/Var Layer in/out: 64 8\n",
      "Decoder Architecture:\n",
      "\tFirst Layer in, out and cond:  8 64 10\n",
      "\tOutput Layer in/out:  64 4000 \n",
      "\n",
      "swav model with l2n init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/28/24 14:15:38 - 0:00:00 - SwavModel(\n",
      "                                       (scpoli_model): scpoli(\n",
      "                                         (embeddings): ModuleList(\n",
      "                                           (0): Embedding(3, 10, max_norm=1.0)\n",
      "                                         )\n",
      "                                         (encoder): Encoder(\n",
      "                                           (FC): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=4000, out_features=64, bias=True)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=64, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (mean_encoder): Linear(in_features=64, out_features=8, bias=True)\n",
      "                                           (log_var_encoder): Linear(in_features=64, out_features=8, bias=True)\n",
      "                                         )\n",
      "                                         (decoder): Decoder(\n",
      "                                           (FirstL): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=8, out_features=64, bias=False)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=64, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (HiddenL): Sequential()\n",
      "                                           (mean_decoder): Sequential(\n",
      "                                             (0): Linear(in_features=64, out_features=4000, bias=True)\n",
      "                                             (1): Softmax(dim=-1)\n",
      "                                           )\n",
      "                                         )\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 08/28/24 14:15:38 - 0:00:00 - Building model done.\n",
      "INFO - 08/28/24 14:15:38 - 0:00:00 - Building optimizer done.\n",
      "INFO - 08/28/24 14:15:38 - 0:00:00 - no mixed precision\n"
     ]
    }
   ],
   "source": [
    "tr= SwAV(dimensionality_reduction= 'pca', augmentation_type='scanpy_knn')\n",
    "tr.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad4a96a-fe9c-490e-a73d-b866366fe8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.pretraining_epochs = 1\n",
    "tr.fine_tuning_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd348640-a13a-41cc-bff5-7fa8072ea18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73cb5213-fbcd-4f19-9e40-a48cbe95af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.ref.label_encoder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828191e9-230f-4ed3-9000-272bc3e2f39e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - 08/28/24 14:15:50 - 0:00:12 - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfatemehs-hashemig\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/wandb/run-20240828_141552-0xsb5d5f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl/runs/0xsb5d5f/workspace' target=\"_blank\">feasible-wave-526</a></strong> to <a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl' target=\"_blank\">https://wandb.ai/fatemehs-hashemig/interpretable-ssl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl/runs/0xsb5d5f/workspace' target=\"_blank\">https://wandb.ai/fatemehs-hashemig/interpretable-ssl/runs/0xsb5d5f/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/28/24 14:16:16 - 0:00:38 - ============ Starting epoch 0 ... ============\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Starting to build k-nearest neighbors graph using Scanpy.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:18 - 0:00:40 - Performing PCA with n_components=50.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:16:28 - 0:00:50 - Running Scanpy neighbors with k=11.\n",
      "INFO - 08/28/24 14:17:17 - 0:01:39 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:21 - 0:01:43 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:21 - 0:01:43 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:24 - 0:01:46 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:24 - 0:01:46 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:26 - 0:01:48 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:28 - 0:01:50 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:30 - 0:01:52 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:30 - 0:01:52 - Processing the kNN graph to extract indices and distances.\n",
      "INFO - 08/28/24 14:17:31 - 0:01:53 - Processing the kNN graph to extract indices and distances.\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "INFO - 08/28/24 14:17:55 - 0:02:17 - Epoch: [0][0]\tTime 98.569 (98.569)\tData 93.148 (93.148)\tLoss 16.3291 (16.3291)\tLr: 0.0000\n",
      "INFO - 08/28/24 14:18:02 - 0:02:24 - Epoch: [0][50]\tTime 0.016 (2.070)\tData 0.000 (1.938)\tLoss 12.9498 (14.9012)\tLr: 0.4293\n",
      "INFO - 08/28/24 14:18:02 - 0:02:25 - Building data done with 2914 images loaded.\n",
      "INFO - 08/28/24 14:18:02 - 0:02:25 - Building optimizer done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0xsb5d5f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.023 MB of 0.023 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cvae</td><td></td></tr><tr><td>loss</td><td></td></tr><tr><td>prot loss</td><td></td></tr><tr><td>swav</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cvae</td><td>966.82221</td></tr><tr><td>loss</td><td>14.71199</td></tr><tr><td>prot loss</td><td>1.24656</td></tr><tr><td>swav</td><td>8.38249</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-wave-526</strong> at: <a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl/runs/0xsb5d5f/workspace' target=\"_blank\">https://wandb.ai/fatemehs-hashemig/interpretable-ssl/runs/0xsb5d5f/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240828_141552-0xsb5d5f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0xsb5d5f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68d998cd2c74f358236406b4d3f069a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0111120462223577, max=1.0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/wandb/run-20240828_141802-44ltllcm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl/runs/44ltllcm/workspace' target=\"_blank\">gallant-water-527</a></strong> to <a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl' target=\"_blank\">https://wandb.ai/fatemehs-hashemig/interpretable-ssl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fatemehs-hashemig/interpretable-ssl/runs/44ltllcm/workspace' target=\"_blank\">https://wandb.ai/fatemehs-hashemig/interpretable-ssl/runs/44ltllcm/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/28/24 14:18:24 - 0:02:46 - ============ Starting epoch 0 ... ============\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "INFO - 08/28/24 14:18:26 - 0:02:48 - Epoch: [0][0]\tTime 1.288 (1.288)\tData 1.272 (1.272)\tLoss 13.4682 (13.4682)\tLr: 0.0000\n"
     ]
    }
   ],
   "source": [
    "tr.train_semi_supervised()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5308ad4-9c95-4776-9fb0-4ebd06f90a4e",
   "metadata": {},
   "source": [
    "# check cell type key effect (on scpoli cvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a41095-ff28-443e-8c36-97751b8e89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import interpretable_ssl.trainers.scpoli_original as scpoli_tr\n",
    "import importlib\n",
    "\n",
    "importlib.reload(scpoli_tr)\n",
    "from interpretable_ssl.trainers.scpoli_original import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3facba27-6e69-4774-a58e-944d320f15a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "new\n"
     ]
    }
   ],
   "source": [
    "trainer = OriginalTrainer(latent_dims=8, batch_size=1024, debug=True, experiment_name = 'scpoli-pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02303529-f64b-4363-b7c6-5d410beb8da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary:\n",
      " \tNum conditions: [3]\n",
      " \tEmbedding dim: [10]\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 64 10\n",
      "\tMean/Var Layer in/out: 64 8\n",
      "Decoder Architecture:\n",
      "\tFirst Layer in, out and cond:  8 64 10\n",
      "\tOutput Layer in/out:  64 4000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby('conditions_combined').first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby(condition_keys).first()\n"
     ]
    }
   ],
   "source": [
    "trainer.cell_type_key = None\n",
    "model = trainer.get_model(trainer.ref.adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5911232c-c8a8-44a5-a3ff-0bcc85938cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import interpretable_ssl.trainers.swav as swav\n",
    "import importlib\n",
    "\n",
    "importlib.reload(swav)\n",
    "from interpretable_ssl.trainers.swav import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1940680-382f-4c6a-bb33-8cb2a61bc028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/28/24 11:56:07 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 08/28/24 11:56:07 - 0:00:00 - all_latent: None\n",
      "                                     augmentation_type: knn\n",
      "                                     base_lr: 4.8\n",
      "                                     batch_size: 512\n",
      "                                     batch_size_version: 2\n",
      "                                     cell_type_key: None\n",
      "                                     checkpoint_freq: 25\n",
      "                                     condition_key: study\n",
      "                                     crops_for_assign: [0, 1]\n",
      "                                     custom_cross_val: False\n",
      "                                     cvae_loss_scaler: 0.0001\n",
      "                                     cvae_reg: 0\n",
      "                                     dataset: pbmc-immune\n",
      "                                     dataset_id: pbmc-immune\n",
      "                                     debug: False\n",
      "                                     default_values: {'dataset_id': 'pbmc-immune', 'model_name_version': 5.0, 'num_prototypes': 128, 'hidden_dim': 64, 'latent_dims': 8, 'batch_size_version': 2, 'batch_size': 512, 'fine_tuning_epochs': 0, 'custom_cross_val': False, 'description': '', 'experiment_name': '', 'condition_key': 'study', 'cell_type_key': 'cell_type', 'linear_eval': False, 'only_eval': False, 'use_early_stopping': False, 'debug': False, 'pretraining_epochs': 300, 'dump_name_version': 4, 'nmb_crops': [8], 'augmentation_type': 'knn', 'size_crops': [224], 'min_scale_crops': [0.14], 'max_scale_crops': [1], 'crops_for_assign': [0, 1], 'temperature': 0.1, 'epsilon': 0.05, 'sinkhorn_iterations': 3, 'feat_dim': 8, 'queue_length': 0, 'epoch_queue_starts': 15, 'base_lr': 4.8, 'final_lr': 0, 'freeze_prototypes_niters': 313, 'wd': 1e-06, 'warmup_epochs': 10, 'start_warmup': 0, 'cvae_reg': 0, 'dist_url': 'env://', 'world_size': -1, 'rank': 0, 'local_rank': 0, 'workers': 10, 'checkpoint_freq': 25, 'use_fp16': False, 'sync_bn': 'pytorch', 'syncbn_process_group_size': 8, 'seed': 31, 'model': '', 'optimizer': '', 'lr_schedule': '', 'queue': None, 'train_loader': '', 'training_stats': '', 'device': 'cuda', 'freezable_prototypes': False, 'cvae_loss_scaler': 0.0001, 'prot_decoding_loss_scaler': 5, 'hidden_mlp': 1024, 'swav_dim': 64, 'use_projector': False, 'model_version': 2, 'train_decoder': False, 'longest_path': 3, 'dimensionality_reduction': None, 'k_neighbors': 10}\n",
      "                                     description: None\n",
      "                                     device: cuda\n",
      "                                     dimensionality_reduction: None\n",
      "                                     dist_url: env://\n",
      "                                     dump_checkpoints: /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-knn8_ep0.05/checkpoints\n",
      "                                     dump_name_version: 4\n",
      "                                     dump_path: /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-knn8_ep0.05\n",
      "                                     epoch_queue_starts: 15\n",
      "                                     epsilon: 0.05\n",
      "                                     experiment_name: swav-all-loss_iloss5_closs0.0001\n",
      "                                     feat_dim: 8\n",
      "                                     final_lr: 0\n",
      "                                     fine_tuning_epochs: 0\n",
      "                                     finetune_ds: None\n",
      "                                     fold: None\n",
      "                                     freezable_prototypes: False\n",
      "                                     freeze_prototypes_niters: 313\n",
      "                                     hidden_dim: 64\n",
      "                                     hidden_mlp: 1024\n",
      "                                     input_dim: 4000\n",
      "                                     k_neighbors: 10\n",
      "                                     latent_dims: 8\n",
      "                                     linear_eval: False\n",
      "                                     local_rank: 0\n",
      "                                     longest_path: 3\n",
      "                                     lr_schedule: None\n",
      "                                     max_scale_crops: [1]\n",
      "                                     min_scale_crops: [0.14]\n",
      "                                     model: None\n",
      "                                     model_name_version: 5.0\n",
      "                                     model_version: 2\n",
      "                                     nmb_crops: [8]\n",
      "                                     nmb_prototypes: 128\n",
      "                                     num_prototypes: 128\n",
      "                                     only_eval: False\n",
      "                                     optimizer: None\n",
      "                                     original_ref: None\n",
      "                                     pretraining_epochs: 300\n",
      "                                     prot_decoding_loss_scaler: 5\n",
      "                                     query: pbmc-immune\n",
      "                                     query_latent: None\n",
      "                                     queue: None\n",
      "                                     queue_length: 0\n",
      "                                     rank: 0\n",
      "                                     ref: pbmc-immune\n",
      "                                     ref_latent: None\n",
      "                                     seed: 31\n",
      "                                     sinkhorn_iterations: 3\n",
      "                                     size_crops: [224]\n",
      "                                     start_warmup: 0\n",
      "                                     swav_dim: 64\n",
      "                                     sync_bn: pytorch\n",
      "                                     syncbn_process_group_size: 8\n",
      "                                     temperature: 0.1\n",
      "                                     train_decoder: False\n",
      "                                     train_loader: None\n",
      "                                     training_stats: None\n",
      "                                     use_early_stopping: False\n",
      "                                     use_fp16: False\n",
      "                                     use_projector: False\n",
      "                                     use_projector_out: False\n",
      "                                     warmup_epochs: 10\n",
      "                                     wd: 1e-06\n",
      "                                     workers: 10\n",
      "                                     world_size: -1\n",
      "INFO - 08/28/24 11:56:07 - 0:00:00 - The experiment will be stored in /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-knn8_ep0.05\n",
      "                                     \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary:\n",
      " \tNum conditions: [3]\n",
      " \tEmbedding dim: [10]\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 64 10\n",
      "\tMean/Var Layer in/out: 64 8\n",
      "Decoder Architecture:\n",
      "\tFirst Layer in, out and cond:  8 64 10\n",
      "\tOutput Layer in/out:  64 4000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby('conditions_combined').first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby(condition_keys).first()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: None",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sw_tr \u001b[38;5;241m=\u001b[39m SwAV(cell_type_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43msw_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/trainers/swav.py:122\u001b[0m, in \u001b[0;36mSwAV.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_stats \u001b[38;5;241m=\u001b[39m initialize_exp(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_scpoli()\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_optimizer()\n",
      "File \u001b[0;32m/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/trainers/swav.py:161\u001b[0m, in \u001b[0;36mSwAV.build_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# why nmb_crops is a list? i used fisrt element but not change it in case needed in furure\u001b[39;00m\n\u001b[1;32m    160\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscpoli_\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_ds \u001b[38;5;241m=\u001b[39m \u001b[43mMultiCropsDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnmb_crops\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentation_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlongest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimensionality_reduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimensionality_reduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondition_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell_type_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_type_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition_encoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondition_encoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditions_combined_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditions_combined_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell_type_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_type_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_ds,\n\u001b[1;32m    177\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    183\u001b[0m )\n\u001b[1;32m    184\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding data done with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/augmenters/adata_augmenter.py:56\u001b[0m, in \u001b[0;36mMultiCropsDataset.__init__\u001b[0;34m(self, adata, n_augmentations, augmentation_type, k_neighbors, longest_path, dimensionality_reduction, n_components, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m=\u001b[39m n_components\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/dataset/scpoli/anndata.py:69\u001b[0m, in \u001b[0;36mMultiConditionAnnotatedDataset.__init__\u001b[0;34m(self, adata, condition_keys, condition_encoders, conditions_combined_encoder, cell_type_keys, cell_type_encoder, labeled_array)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cell_type_key \u001b[38;5;129;01min\u001b[39;00m cell_type_keys:\n\u001b[0;32m---> 69\u001b[0m     level_cell_types \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_type_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcondition_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_type_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_types\u001b[38;5;241m.\u001b[39mappend(level_cell_types)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_types \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_types)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/dataset/scpoli/_utils.py:41\u001b[0m, in \u001b[0;36mlabel_encoder\u001b[0;34m(adata, encoder, condition_key)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlabel_encoder\u001b[39m(adata, encoder, condition_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode labels of Annotated `adata` matrix.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m       Parameters\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m       ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m            dictionary with labels and encoded labels as key, value pairs.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     unique_conditions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(\u001b[43madata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcondition_key\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     42\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(adata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(unique_conditions)\u001b[38;5;241m.\u001b[39missubset(\u001b[38;5;28mset\u001b[39m(encoder\u001b[38;5;241m.\u001b[39mkeys())):\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "sw_tr = SwAV(cell_type_key = None)\n",
    "sw_tr.setup()\n",
    "batch = next(iter(sw_tr.train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2b4684e-d2bd-428d-a4f4-43494131ff4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msw_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m batch\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "390da1dd-6137-4c73-bb08-7bf8ae83ba56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SwAV' object has no attribute 'train_ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msw_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ds\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SwAV' object has no attribute 'train_ds'"
     ]
    }
   ],
   "source": [
    "sw_tr.train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ddebc-8fc5-4e0d-a604-c8c6500f89de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
